\documentclass[12pt, a4paper]{article}
\usepackage{natbib}
\usepackage{vmargin}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{subfigure}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{subfiles}
\usepackage{subfigure}
\usepackage{framed}
\usepackage{subfiles}
\usepackage{amsbsy}
\usepackage{amsthm, amsmath}
%\usepackage[dvips]{graphicx}
\bibliographystyle{chicago}
\renewcommand{\baselinestretch}{1.1}

% left top textwidth textheight headheight % headsep footheight footskip
\setmargins{3.0cm}{2.5cm}{15.5 cm}{23.5cm}{0.25cm}{0cm}{0.5cm}{0.5cm}

\pagenumbering{arabic}
%-------------------------------------------------------------------Simplifying GLS by KH -%


\begin{document}











%--------------------------------------------------------------------%


\section{The extended likelihood}
\begin{verbatim}
The desire to have an entirely likelihood-based justification for estimates of random effects, in contrast to Henderson's equation, has motivated \citet[page 429]{Pawi:in:2001} to define the \emph{extended likelihood}. He remarks ``In mixed effects modelling the extended likelihood has been called \emph{h-likelihood} (for hierarchical  likelihood) by \cite{Lee:Neld:hier:1996}, while in smoothing literature it is known as the \emph{penalized likelihood} (e.g.\ \citeauthor{Gree:Silv:nonp:1994} \citeyear{Gree:Silv:nonp:1994})." The extended likelihood can be written $L(\beta,\theta,b|y) = p(y|b;\beta,\theta) p(b;\theta)$ and adopting the same distributional assumptions used by \cite{Henderson:1950} yields the log-likelihood function

\begin{eqnarray*}
\ell_h(\beta,\theta,b|y)
& = \displaystyle -\frac{1}{2} \left\{ \log|\Sigma| + (y - X \beta -Zb)'\Sigma^{-1}( y - X \beta -Zb) \right.\\
&  \hspace{0.5in} \left. + \log|D| + b^\prime D^{-1}b \right\}.
\end{eqnarray*}
Given $\theta$, differentiating with respect to $\beta$ and $b$ returns Henderson's equations in (\ref{Henderson:Equations}).

\subsubsection{The LME model as a general linear model}
Henderson's equations in (\ref{Henderson:Equations}) can be rewritten $( T^\prime W^{-1} T ) \delta = T^\prime W^{-1} y_{a} $ using
\[
\delta = \begin{pmatrix}{\beta \cr b},
\ y_{a} = \begin{pmatrix}{
y \cr \psi
},
\ T = \begin{pmatrix}{
X & Z  \cr
0 & I
},
\ \textrm{and} \ W = \begin{pmatrix}{
\Sigma & 0  \cr
0 &  D },
\]
where \cite{Lee:Neld:Pawi:2006} describe $\psi = 0$ as quasi-data with mean $\mathrm{E}(\psi) = b.$ Their formulation suggests that the joint estimation of the coefficients $\beta$ and $b$ of the linear mixed effects model can be derived via a classical augmented general linear model $y_{a} = T\delta + \varepsilon$ where $\mathrm{E}(\varepsilon) = 0$ and $\mathrm{var}(\varepsilon) = W,$ with \emph{both} $\beta$ and $b$ appearing as fixed parameters. The usefulness of this reformulation of an LME as a general linear model will be revisited.

\end{verbatim}


%---------------------------------------------------------%
\newpage
\section{Lesaffre's paper.} %5.6

Lesaffre considers the case-weight perturbation approach.


%\citep{cook86}
Cook's 86 describes a local approach wherein each case is given a weight $w_{i}$ and the effect on the parameter estimation is measured by perturbing these weights. Choosing weights close to zero or one corresponds to the global case-deletion approach.

Lesaffre  describes the displacement in log-likelihood as a useful metric to evaluate local influence %\citep{cook86}.


%\citet{lesaffre}
Lesaffre describes a framework to detect outlying observations that matter in an LME model. Detection should be carried out by evaluating diagnostics $C_{i}$ , $C_{i}(\alpha)$ and $C_{i}(D,\sigma^2)$.


Lesaffre defines the total local influence of individual $i$ as
\begin{equation}
C_{i} = 2 | \triangle \prime _{i} L^{-1} \triangle_{i}|.
\end{equation}


The influence function of the MLEs evaluated at the $i$th point $IF_{i}$, given by
\begin{equation}
IF_{i} = -L^{-1}\triangle _{i}
\end{equation}
can indicate how $\hat{theta}$ changes as the weight of the $i$th
subject changes.

The manner by which influential observations distort the estimation process can be determined by inspecting the
interpretable components in the decomposition of the above measures of local influence.


Lesaffre comments that there is no clear way of interpreting the information contained in the angles, but that this doesn't mean the information should be ignored.


















\bibliographystyle{chicago}
\bibliography{DB-txfrbib}
\section{Lesaffre's paper.} %5.6

Lesaffre considers the case-weight perturbation approach.


%\citep{cook86}
Cook's 86 describes a local approach wherein each case is given a weight $w_{i}$ and the effect on the parameter estimation is measured by perturbing these weights. Choosing weights close to zero or one corresponds to the global case-deletion approach.

Lesaffre  describes the displacement in log-likelihood as a useful metric to evaluate local influence %\citep{cook86}.


%\citet{lesaffre}
Lesaffre describes a framework to detect outlying observations that matter in an LME model. Detection should be carried out by evaluating diagnostics $C_{i}$ , $C_{i}(\alpha)$ and $C_{i}(D,\sigma^2)$.


Lesaffre defines the total local influence of individual $i$ as
\begin{equation}
C_{i} = 2 | \triangle \prime _{i} L^{-1} \triangle_{i}|.
\end{equation}


The influence function of the MLEs evaluated at the $i$th point $IF_{i}$, given by
\begin{equation}
IF_{i} = -L^{-1}\triangle _{i}
\end{equation}
can indicate how $\hat{theta}$ changes as the weight of the $i$th
subject changes.

The manner by which influential observations distort the estimation process can be determined by inspecting the
interpretable components in the decomposition of the above measures of local influence.


Lesaffre comments that there is no clear way of interpreting the information contained in the angles, but that this doesn't mean the information should be ignored.


%-------------------------------------------------------------------------------------------------------%

\end{document}
