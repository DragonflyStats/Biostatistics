\section{Introduction to Mixed Models}

%\citet{BrownPrescott} defines random effects as realizations of
%samples from a normal distribution with mean equal to zero.

All models are characterized by the mean $\alpha$ and the error
terms. In addition to these terms, any model described so far will
have either random effects terms or fixed effects terms and
accordingly are referred to as random or fixed models. Models that
have both fixed effects terms and random effects terms are known
as 'mixed effects models'. Once the theory underlying fixed and
random effects models has been fully understood, the progression
to understanding mixed models is very simple.

Elaborating on the original mice litter example, the six litters
by each mouse were fed according to three different dietary
treatments \citep{Searle}. Therefore a fixed effect $\phi_{j}$ has
been added to the model, which is now formulated as follows;
\begin{equation}
y_{ij} = \mu + \delta_{i} + \phi_{j} + \gamma_{ij} +
\epsilon_{ijk}
\end{equation}
As before, an interaction effect $\gamma_{ij}$ must also be added
to the model. In cases where the interaction term describes the
combined effect of fixed and random components, it should be
treated as random effect. The variance of the above model is
composed of the $\sigma^{2}_{\delta}$, $\sigma^{2}_{\gamma}$ and
$\sigma^{2}_{\epsilon}$ .


It may be shown that the interaction factors make no contribution
to the outcome, i.e $\gamma_{ij}$ is consistently calculated as
zero. Considering the skin tumour example, a person's age would
bear no relation to their gender and hence there would be
plausible interaction between the two factors. Indeed , in keeping
with the `Law of Parsimony', factors should be specified such that
each would convey separate information. However, interaction terms
are extant when the model specifies repeated observations, as
there is necessarily a relationship between observations from the
same subject. Importantly, interaction effects, being random
effects, are attended by variance component terms and therefore
also contribute to the overall variance of the model.

\citet{Searle} gives a mixed effects model formulation for the
Grubbs artillery study. $y_{ij}$ is the muzzle velocity of the
$i$th shell, as measured by the $j$th chronometer.
\begin{equation}
y_{ij} = \mu + \alpha_{i} + \beta_{j}  + \epsilon_{ij}
\end{equation}
In this formulation $\alpha_{i}$ is the random effect of round
$i$, and the fixed effect component $\beta_{j}$ is the bias in
chronometer $j$. (Also, no interaction term is used).




\section{Matrix Formulation} There are matrix (i.e multivariate)
formulations of both fixed effects models and random effects
models. \citet{BrownPrescott} remarks that the matrix notation
makes the underlying theory of mixed effects models much easier to
work with. The fixed effects models can be specified as follows;

\begin{equation}
\textbf{Y} = \textbf{Xb} + \textbf{e}
\end{equation}

\textbf{Y} is the vector of $n$ observations, with dimension $n
\times 1$. \textbf{b} is a vector of fixed $p$ effects, and has
dimension $p \times 1$. It is composed of coefficients, with the
first element being the population mean. For the skin tumour
example, with the three specified fixed effects, $p=4$. \textbf{X}
is known as the design `matrix', model matrix for fixed effects,
and comprises $0$s or $1$s, depending on whether the relevant
fixed effects have any effect on the observation is question.
\textbf{X} has dimension $n \times p$. \textbf{e} is the vector of
residuals with dimension $n \times 1$.

The random effects models can be specified similarly. \textbf{Z}
is known as the `model matrix for random effects', and also
comprises $0$s or $1$s. It has dimension $n \times q$. \textbf{u}
is a vector of random $q$ effects, and has dimension $q \times 1$.

\begin{equation}
\textbf{Y} = \textbf{Zu} + \textbf{e}
\end{equation}

Again, once the component fixed effects and random effects
components are considered, progression to a mixed model
formulation is a simple step. Further to \citet{LW82}, it is
conventional to formulate a mixed effects model in matrix form as
follows:

\begin{equation}
\textbf{Y} = \textbf{Xb} + \textbf{Zu} + \textbf{e}
\end{equation}

($E(\textbf{u})=0$, $E(\textbf{e})=0 $ and $E(\textbf{y}) =
\textbf{Xb}$)


\section{Statement of the LME model}
A linear mixed effects model is a linear mdoel that combined fixed and random effect terms formulated by \citet{LW82} as follows;

\begin{displaymath}
Y_{i} =X_{i}\beta + Z_{i}b_{i} + \epsilon_{i}
\end{displaymath}
\begin{itemize}
	
	\item $Y_{i}$ is the $n \times 1$ response vector \item $X_{i}$ is
	the $n \times p$ Model matrix for fixed effects \item $\beta$ is
	the $p \times 1$ vector of fixed effects coefficients \item
	$Z_{i}$ is the $n \times q$ Model matrix for random effects \item
	$b_{i}$ is the $q \times 1$ vector of random effects coefficients,
	sometimes denoted as $u_{i}$ \item $\epsilon$ is the $n \times 1$
	vector of observation errors
\end{itemize}


The linear mixed effects model is given by
\begin{equation}
Y = X\beta + Zu + \epsilon
\end{equation}
\section{Standard Deviation of Differences}
In computing limits of agreement, it is first necessary to have an estimate for the standard deviations of the differences. When the agreement of two methods is analyzed using LME models, a clear method of how to compute the standard deviation is required. As the estimate for inter-method bias and the quantile would be the same for both methodologies, the focus hereon is solely on the variance of differences.

The standard deviation of the differences of methods $x$ and $y$ is computed using values from the overall VC matrix.
\[
\mbox{Var}(x - y ) = \mbox{Var} ( x )  + \mbox{Var} ( y ) - 2\mbox{Cov} ( x ,y )
\]





\section{Extended LME model}
% Pinheiro Bates Page 202
The extended single level LME model relaxes the independence assumption, allowing heteroscedastic and correlated within group errors.


\begin{equation}
\epsilon_{i} = \mathcal{N}(0, \sigma^2 \Lambda_{i})
\end{equation}

$\Lambda_{i}$ are positive definite matrices. $\sigma^2$ is factored out of the matrix for computational reasons.


\section{Restricted Likelihood Estimation}

Restricted maximum likelihood (REML) is an alternative methods of
computing parameter estimated, developed by \citet*{PT71} and
\citet{Harville} to provide unbiased estimates of variance and
covariance parameters. The REML approach does not base estimates on a maximum likelihood fit of all the information, but instead uses a likelihood function derived from a data set, transformed to remove the irrelevant influences \citep{REMLDefine}.
%		This particular form of maximum likelihood estimation which does not base estimates on a maximum likelihood fit of all the information, but instead uses a likelihood function calculated from a transformed set of data, so that nuisance parameters have no effect.
%		

REML obtains estimates of the fixed effects using non-likelihoodlike methods, such as ordinary least squares or generalized least squares, and then using these estimates it
maximizes the likelihood of the residuals (subtracting off the
fixed effects) to obtain estimates of the variance parameters. In
most software packages REML is the default algorithm used to
compute coefficients for the predictor variables. REML estimation
reduces the bias in the variance component, and also handles high
correlations more effectively, and is less sensitive to outliers
than ML.

The variance components in the LME model may be estimated by ML or REML. Maximum Likelihood estimates do not take into account the estimation of fixed effects and so
are biased downwards. REML estimates accounts for the presence of these nuisance parameters by maximising the linearly independent error contrasts to obtain more unbiased estimates.




%The log likelihood $\emph{l}(\theta)$



\citet{McCullSearle} describes two important outcomes of using
REML. Firstly variance components can be estimated without being
affected by fixed effects. Secondly in estimating variance
components with REML, degrees of freedom for the fixed effects can
be taken into account implicitly, whereas with ML they are not.
%	When estimating variance from normally distributed data, the ML
%	estimator for $\sigma^{2}$ is $\frac{S_{yy}}{n}$ whereas the REML
%	estimator is $\frac{S_{yy}}{n-1}$. $S_{yy}$ is the sum of square
%	identity;
%	\begin{equation}
%	S_{yy} = \Sigma_{i=i}^{N} (y-\bar{y})^{2}
%	\end{equation}

Restricted maximum likelihood is often preferred to maximum likelihood because REML estimation reduces the bias in the variance component by taking into account the loss of degrees of freedom that results
from estimating the fixed effects in $\boldsymbol{\beta}$. Restricted maximum likelihood also handles high correlations more effectively, and is less sensitive to outliers than maximum likelihood.  The problem with REML for model building is that the likelihoods obtained for different fixed effects are not comparable. Hence it is not valid to compare models with different fixed effects using a likelihood ratio test or AIC when REML is used to
estimate the model. Therefore models derived using ML must be used instead.


%\subsection{Model Selection} The previous section on estimation assumes the specification of a mixed model in terms of X, Z, D, and R. Even though $X$ and $Z$ have known elements, there is some
%flexibility is specifying the form and construction is flexible, and for a particular data set, there are numerous possibilities	that can be considered. Similarly, various potential covariance 	structures for \textbf{D} and \textbf{R} may be considered.


%------------------------------------------------------------------------------%





\section{nlme - Variance functions}

Variance functions are applied to LME models through the \texttt{`weights'} argument. $R$ supports several variance functions.

`\texttt{varIdent}' cosntructs a model with different variances per stratum.

\subsection{Diagnostic plots}
[Pinheiro Bates Page 391] Diagnostic plots for identifying within-group heteroscedascity and assessing the adequacy of a variance function can also be used with `nlme' objects.




\section{Likelihood and estimation}

Likelihood is the hypothetical probability that an event that has already occurred would yield a specific outcome. Likelihood differs from probability in that probability refers to future occurrences, while likelihood refers to past known outcomes.

The likelihood function ($L(\theta)$)is a fundamental concept in statistical inference. It indicates how likely a particular population is to produce an observed sample. The set of values that maximize the likelihood function are considered to be optimal, and are used as the estimates of the parameters. For computational ease, it is common to use the logarithm of the likelihood function, known simply as the log-likelihood ($\ell(\theta)$).

%========================================================= %

\newpage

\section{Linear Mixed effects Models}
A linear mixed effects (LME) model is a statistical model containing both fixed effects and random effects (random effects are also known as variance components). LME models are a generalization of the classical linear model, which contain fixed effects only. When the levels of factors are considered to be sampled from a population,
and each level is not of particular interest, they are considered random quantities with associated variances.
The effects of the levels, as described, are known as random effects. Random effects are represented by unobservable
normally distributed random variables. Conversely fixed effects are considered non-random and the
levels of each factor are of specific interest.
%LME models are useful models when considering repeated measurements or grouped observations.

\citet{Fisher4} introduced variance components models for use in genetical studies. Whereas an estimate for variance must take an non-negative value, an individual variance component, i.e.\ a component of the overall variance, may be negative.

The methodology has developed since, including contributions from
\citet{tippett}, who extend the use of variance components into linear models, and \citet{eisenhart}, who introduced the `mixed model' terminology and formally distinguished between mixed and random effects models. \citet{Henderson:1950} devised a methodology for deriving estimates for both the fixed effects and the random effects, using a set of equations that would become known as `mixed model equations' or `Henderson's equations'.
LME methodology is further enhanced by Henderson's later works \citep{Henderson53, Henderson59,Henderson63,Henderson73,Henderson84a}. The key features of Henderson's work provide the basis for the estimation techniques.

\citet{HartleyRao} demonstrated that unique estimates of the variance components could be obtained using maximum likelihood methods. However these estimates are known to be biased `downwards' (i.e.\ underestimated) , because of the assumption that the fixed estimates are known, rather than being estimated from the data. \citet{PattersonThompson} produced an alternative set of estimates, known as the restricted maximum likelihood (REML) estimates, that do not require the fixed effects to be known. Thusly there is a distinction the REML estimates and the original estimates, now commonly referred to as ML estimates.

\citet{LW82} provides a form of notation for notation for LME models that has since become the standard form, or the basis for more complex formulations. Due to computation complexity, linear mixed effects models have not seen widespread use until many well known statistical software applications began facilitating them. SAS Institute added PROC MIXED to its software suite in 1992 \citep{singer}. \citet{PB} described how to compute LME models in the \texttt{S-plus} environment.

Using Laird-Ware form, the LME model is commonly described in matrix form,
\begin{equation}
y = X\beta + Zb + \epsilon
\label{LW}
\end{equation}

\noindent where $y$ is a vector of $N$ observable random variables, $\beta$ is a vector of $p$ fixed effects, $X$ and $Z$ are $N \times p$ and $N \times q$ known matrices, and $b$ and $\epsilon$  are vectors of $q$ and $N,$ respectively, random effects such that $\mathrm{E}(b)=0, \ \mathrm{E}(\epsilon)=0$
and
%	\[
%	\mathrm{var}
%	\begin{pmatrix}{
%		b \cr
%		\epsilon }  =
%	\begin{pmatrix}{
%		D & 0 \cr
%		0 & \Sigma }
%	\]
where $D$ and $\Sigma$ are positive definite matrices parameterized by an unknown variance component parameter vector $ \theta.$ The variance-covariance matrix for the vector of observations $y$ is given by $V = ZDZ^{\prime}+ \Sigma.$ This implies $y \sim(X\beta, V) = (X\beta,ZDZ^{\prime}+ \Sigma)$. It is worth noting that $V$ is an $n \times n$ matrix, as the dimensionality becomes relevant later on. The notation provided here is generic, and will be adapted to accord with complex formulations that will be encountered in due course.

%\subsection{Likelihood and estimation}

% Likelihood is the hypothetical probability that an event that has already occurred would yield a specific outcome. Likelihood differs from probability in that probability refers to future occurrences, while likelihood refers to past known outcomes.

% The likelihood function ($L(\theta)$)is a fundamental concept in statistical inference. It indicates how likely a particular population is to produce an observed sample. The set of values that maximize the likelihood function are considered to be optimal, and are used as the estimates of the parameters. For computational ease, it is common to use the logarithm of the likelihood function, known simply as the log-likelihood ($\ell(\theta)$).


\subsection{Estimation}
Estimation of LME models involve two complementary estimation issues'; estimating the vectors of the fixed and random effects estimates $\hat{\beta}$ and $\hat{b}$ and estimating the variance covariance matrices $D$ and $\Sigma$.
Inference about fixed effects have become known as `estimates', while inferences about random effects have become known as `predictions'. The most common approach to obtain estimators are Best Linear Unbiased Estimator (BLUE) and Best Linear Unbiased Predictor (BLUP). For an LME model given by (\ref{LW}), the BLUE of $\hat{\beta}$ is given by
\[\hat{\beta} = (X^\prime V^{-1}X)^{-1}X^\prime V^{-1}y,\]whereas the BLUP of $\hat{b}$ is given by
\[\hat{b} = DZ^{\prime} V^{-1} (y-X\hat{\beta}).\]



\subsubsection{Estimation of the fixed parameters}

The vector $y$ has marginal density $y \sim \mathrm{N}(X \beta,V),$ where $V = \Sigma + ZDZ^\prime$ is specified through the variance component parameters $\theta.$ The log-likelihood of the fixed parameters $(\beta, \theta)$ is
\begin{equation}
\ell (\beta, \theta|y) =
-\frac{1}{2} \log |V| -\frac{1}{2}(y -
X \beta)'V^{-1}(y -
X \beta), \label{Likelihood:MarginalModel}
\end{equation}
and for fixed $\theta$ the estimate $\hat{\beta}$ of $\beta$ is obtained as the solution of
\begin{equation}
(X^\prime V^{-1}X) {\beta} = X^\prime V^{-1}y.
\label{mle:beta:hat}
\end{equation}

Substituting $\hat{\beta}$ from (\ref{mle:beta:hat}) into $\ell(\beta, \theta|y)$ from (\ref{Likelihood:MarginalModel}) returns the \emph{profile} log-likelihood
\begin{eqnarray*}
	\ell_P(\theta \mid y) &=& \ell(\hat{\beta}, \theta \mid y) \\
	&=& -\frac{1}{2} \log |V| -\frac{1}{2}(y - X \hat{\beta})'V^{-1}(y - X \hat{\beta})
\end{eqnarray*}
of the variance parameter $\theta.$ Estimates of the parameters $\theta$ specifying $V$ can be found by maximizing $\ell_P(\theta \mid y)$ over $\theta.$ These are the ML estimates.

For REML estimation the \emph{restricted} log-likelihood is defined as
\[
\ell_R(\theta \mid y) =
\ell_P(\theta \mid y) -\frac{1}{2} \log |X^\prime VX |.
\]
%\subsubsection{Likelihood estimation techniques}
%Maximum likelihood and restricted maximum likelihood have become the most common strategies
%for estimating the variance component parameter $\theta.$ Maximum likelihood estimation obtains
%parameter estimates by optimizing the likelihood function.
%To obtain ML estimate the likelihood is constructed as a function of the parameters in the specified LME model.
% The maximum likelihood estimates (MLEs) of the parameters are the values of the arguments that maximize the likelihood function.

The REML approach does not base estimates on a maximum likelihood fit of all the information, but instead uses a likelihood function derived from a data set, transformed to remove the irrelevant influences \citep{REMLDefine}.
Restricted maximum likelihood is often preferred to maximum likelihood because REML estimation reduces the bias in the variance component by taking into account the loss of degrees of freedom that results
from estimating the fixed effects in $\boldsymbol{\beta}$. Restricted maximum likelihood also handles high correlations more effectively, and is less sensitive to outliers than maximum likelihood.  The problem with REML for model building is that the likelihoods obtained for different fixed effects are not comparable. Hence it is not valid to compare models with different fixed effects using a likelihood ratio test or AIC when REML is used to
estimate the model. Therefore models derived using ML must be used instead.

\subsubsection{Estimation of the random effects}

The established approach for estimating the random effects is to use the best linear predictor of $b$ from $y,$ which for a given $\beta$ equals $DZ^\prime V^{-1}(y - X \beta).$ In practice $\beta$ is replaced by an estimator such as $\hat{\beta}$ from (\ref{mle:beta:hat}) so that $\hat{b} = DZ^\prime V^{-1}(y - X \hat{\beta}).$ Pre-multiplying by the appropriate matrices it is straightforward to show that these estimates $\hat{\beta}$ and $\hat{b}$ satisfy the equations in (\ref{Henderson:Equations}).

\subsubsection{Algorithms for likelihood function optimization}Iterative numerical techniques are used to optimize the log-likelihood function and estimate the covariance parameters $\theta$. The procedure is subject to the constraint that $R$ and $D$ are both positive definite. The most common iterative algorithms for optimizing the likelihood function are the Newton-Raphson method, which is the preferred method, the expectation maximization (EM) algorithm and the Fisher scoring methods.

The EM algorithm, introduced by \citet{EM}, is an iterative technique for maximizing complicated likelihood functions. The algorithm alternates between performing an expectation (E) step
and the maximization (M) step. The `E' step computes the expectation of the log-likelihood evaluated using the current
estimate for the variables. In the `M' step, parameters that maximize the expected log-likelihood, found on the previous `E' step, are computed. These parameter estimates are then used to determine the distribution of the variables in the next `E' step. The algorithm alternatives between these two steps until convergence is reached.

The main drawback of the EM algorithm is its slow rate of
convergence. Consequently the EM algorithm is rarely used entirely in LME estimation,
instead providing an initial set of values that can be passed to
other optimization techniques.

The Newton Raphson (NR) method is the most common, and recommended technique for ML and
REML estimation. The NR algorithm minimizes an objective function defines as $-2$ times the log likelihood for the covariance parameters $\theta$. At every iteration the NR algorithm requires the
calculation of a vector of partial derivatives, known as the gradient, and the second derivative matrix with respect to the covariance parameters. This is known as the observed Hessian matrix. Due to the Hessian matrix, the NR algorithm is more time-consuming, but convergence is reached with fewer iterations compared to the EM algorithm. The Fisher scoring algorithm is an variant of the NR algorithm that is more numerically stable and likely to converge, but not recommended to obtain final estimates.


%------------------------------------------------------------------------------%
\subsection{Formulation of the response vector}
Information of individual $i$ is recorded in a response vector $\boldsymbol{y}_{i}$. The response vector is constructed by stacking the response of the $2$ responses at the first instance, then the $2$ responses at the second instance, and so on. Therefore the response vector is a $2n_{i} \times 1$ column vector.
The covariance matrix of $\boldsymbol{y_{i}}$ is a $2n_{i} \times 2n_{i}$ positive definite matrix $\boldsymbol{\Omega}_{i}$.

Consider the case where three measurements are taken by both methods $A$ and $B$, $\boldsymbol{y}_{i}$ is a $6 \times 1$ random vector describing the $i$th subject.
\[
\boldsymbol{y}_{i} = (y_{i}^{A1},y_{i}^{B1},y_{i}^{A2},y_{i}^{B2},y_{i}^{A3},y_{i}^{B3}) \prime
\]

The response vector $\boldsymbol{y_{i}}$ can be formulated as an LME model according to Laird-Ware form.
\begin{eqnarray*}
	\boldsymbol{y_{i}} = \boldsymbol{X_{i}\beta}  + \boldsymbol{Z_{i}b_{i}} + \boldsymbol{\epsilon_{i}}\\
	\boldsymbol{b_{i}} \sim \mathcal{N}(\boldsymbol{0,D})\\
	\boldsymbol{\epsilon_{i}} \sim \mathcal{N}(\boldsymbol{0,R_{i}})
\end{eqnarray*}

Information on the fixed effects are contained in a three dimensional vector $\boldsymbol{\beta} = (\beta_{0},\beta_{1},\beta_{2})\prime$. For computational purposes $\beta_{2}$ is conventionally set to zero. Consequently $\boldsymbol{\beta}$ is the solutions of the means of the two methods, i.e. $E(\boldsymbol{y}_{i})  = \boldsymbol{X}_{i}\boldsymbol{\beta}$. The variance covariance matrix $\boldsymbol{D}$ is a general $2 \times 2$ matrix, while $\boldsymbol{R}_{i}$ is a $2n_{i} \times 2n_{i}$ matrix.

%------------------------------------------------------------------------------%
\subsection{Decomposition of the response covariance matrix}

The variance covariance structure can be re-expressed in the following form,
\[
\mbox{Cov}(\mbox{y}_{i}) = \boldsymbol{\Omega_{i}} = \boldsymbol{Z}_{i}\boldsymbol{D}\boldsymbol{Z}_{i}^\prime + \boldsymbol{R_{i}}.
\]

$\boldsymbol{R_{i}}$ can be shown to be the Kronecker product of a correlation matrix $\boldsymbol{V}$ and $\boldsymbol{\Lambda}$. The correlation matrix $\boldsymbol{V}$ of the repeated measures on a given response variable is assumed to be the same for all response variables. Both \citet{hamlett} and \citet{lam} use the identity matrix, with dimensions $n_{i} \times n_{i}$ as the formulation for $\boldsymbol{V}$. \citet{ARoy2009} remarks that, with repeated measures, the response for each subject is correlated for each variable, and that such correlation must be taken into account in order to produce a valid inference on correlation estimates.  \citet{ARoy20092006} proposes various correlation structures may be assumed for repeated measure correlations, such as the compound symmetry and autoregressive structures, as alternative to the identity matrix.

However, for the purposes of method comparison studies, the necessary estimates are currently only determinable when the identity matrix is specified, and the results in \citet{ARoy2009} indicate its use.

For the response vector described, \citet{hamlett} presents a detailed covariance matrix. A brief summary shall be presented here only. The overall variance matrix is a $6 \times 6$ matrix composed of two types of $2 \times 2$ blocks. Each block represents one separate time of measurement.

\[
\boldsymbol{\Omega}_{i} = \left(
\begin{array}{ccc}
\boldsymbol{\Sigma} & \boldsymbol{D} & \boldsymbol{D}\\
\boldsymbol{D} & \boldsymbol{\Sigma} & \boldsymbol{D}\\
\boldsymbol{D} & \boldsymbol{D} & \boldsymbol{\Sigma}\\
\end{array}\right)
\]

The diagonal blocks are $\Sigma$, as described previously. The $2 \times 2$ block diagonal matrix in $\boldsymbol{\Omega}$ gives $\boldsymbol{\Sigma}$. $\boldsymbol{\Sigma}$ is the sum of the between-subject variability $\boldsymbol{D}$ and the within subject variability $\boldsymbol{\Lambda}$.

$\boldsymbol{\Omega_{i}}$ can be expressed as
\[
\boldsymbol{\Omega_{i}} = \boldsymbol{Z}_{i}\boldsymbol{D}\boldsymbol{Z}_{i}^\prime + ({\boldsymbol{I_{n_{i}}} \otimes \boldsymbol{\Lambda}}).
\]
The notation $\mbox{dim}_{n_{i}}$ means an $n_{i} \times n_{i}$ diagonal block.

\subsection{Correlation terms}
\citet{hamlett} demonstrated how the between-subject and within subject variabilities can be expressed in terms of
correlation terms.

\[
\boldsymbol{D} = \left( \begin{array}{cc}
\sigma^2_{A}\rho_{A} & \sigma_{A}\sigma_{b}\rho_{AB}\delta \\
\sigma_{A}\sigma_{b}\rho_{AB}\delta & \sigma^2_{B}\rho_{B}\\

\end{array}\right)
\]

\[
\boldsymbol{\Lambda} = \left(
\begin{array}{cc}
\sigma^2_{A}(1-\rho_{A}) & \sigma_{AB}(1-\delta)  \\
\sigma_{AB}(1-\delta) & \sigma^2_{B}(1-\rho_{B}) \\
\end{array}\right).
\]

$\rho_{A}$ describe the correlations of measurements made by the method $A$ at different times. Similarly $\rho_{B}$ describe the correlation of measurements made by the method $B$ at different times. Correlations among repeated measures within the same method are known as intra-class correlation coefficients. $\rho_{AB}$ describes the correlation of measurements taken at the same same time by both methods. The coefficient $\delta$ is added for when the measurements are taken at different times, and is a constant of less than $1$ for linked replicates. This is based on the assumption that linked replicates measurements taken at the same time would have greater correlation than those taken at different times. For unlinked replicates $\delta$ is simply $1$. \citet{hamlett} provides a useful graphical depiction of the role of each correlation coefficients.
















\subsection{For Expository Purposes}



For expository purposes consider the case where each item provides three replicates by each method. Then in matrix notation the model has the structure
\[
\boldsymbol{y}_{i} =
\left(
\begin{array}{c}
y_{1i1} \\
y_{2i1} \\
y_{1i2} \\
y_{2i2} \\
y_{1i3} \\
y_{2i3} \\
\end{array}
\right) = 
\left(
\begin{array}{ccc}
1 & 1 & 0 \\
1 & 0 & 1 \\
1 & 1 & 0 \\
1 & 0 & 1 \\
1 & 1 & 0 \\
1 & 0 & 1 \\
\end{array}
\right)
\left(
\begin{array}{c}
\beta_0 \\ \beta_1 \\ \beta_2 \\
\end{array}
\right)
+
\left(
\begin{array}{cc}
1 & 0 \\
0 & 1 \\
1 & 0 \\
0 & 1 \\
1 & 0 \\
0 & 1 \\
\end{array}
\right)\left(
\begin{array}{c}
b_{1i} \\   b_{2i} \\
\end{array}
\right)
+
\left(
\begin{array}{c}
\epsilon_{1i1} \\
\epsilon_{2i1} \\
\epsilon_{1i2} \\
\epsilon_{2i2} \\
\epsilon_{1i3} \\
\epsilon_{2i3} \\
\end{array}
\right).
\]
The between item variance covariance $\boldsymbol{G}$ is as before, while the within item variance covariance is given as
%------Specification of within item VC matrix R---%
\[ \boldsymbol{G} =\left(
\begin{array}{cc}
g^2_1  & g_{12} \\
g_{12} & g^2_2 \\
\end{array}
\right) \]

\[
\boldsymbol{R}_i = \left(
\begin{array}{cccccc}
\sigma^2_{1} & \sigma_{12} & 0 & 0 & 0 & 0 \\
\sigma_{12} & \sigma^2_{2} & 0 & 0 & 0 & 0 \\
0 & 0 & \sigma^2_{1} & \sigma_{12} & 0 & 0 \\
0 & 0 & \sigma_{12} & \sigma^2_{2} & 0 & 0 \\
0 & 0 & 0 & 0 & \sigma^2_{1} & \sigma_{12} \\
0 & 0 & 0 & 0 & \sigma_{12} & \sigma^2_{2} \\
\end{array} \right)
\]
Assumptions made on the structures of $\boldsymbol{G}$ and $\boldsymbol{R}_i$ will be discussed in due course.












%-----------------------------------------------------------------------------------------------------%



\newpage

\section{Linear mixed effects models}

% http://www.artifex.org/~meiercl/R_statistics_guide.pdf
These models are used when there are both fixed and random effects that need to be incorporated into a model.

Fixed effects usually correspond to experimental treatments for which one has data for the entire population of samples corresponding to that treatment.

Random effects,on the other hand, are assigned in the case where we have measurements on a group of samples, and those
samples are taken from some larger sample pool, and are presumed to be representative.

As such, linear mixed effects models treat the error for fixed effects differently than the error for random effects.


\subsection{Stating the LME Model}
The general linear mixed
model is
\[
Y = X\beta + Zu + \varepsilon\]
where Y is a $(n\times1)$ vector of observed data, X is an $(n\times p)$ fixed-effects design or regressor matrix of rank
k, Z is a $(n \times g)$ random-effects design or regressor matrix, $u$ is a $(g \times 1)$ vector of random effects, and $\varepsilon$ is
an $(n\times1)$ vector of model errors (also random effects). The distributional assumptions made by the MIXED
procedure are as follows: γ is normal with mean 0 and variance G; $\varepsilon$ is normal with mean 0 and variance
R; the random components $u$ and $\varepsilon$ are independent. Parameters of this model are the fixed-effects β and
all unknowns in the variance matrices G and R. The unknown variance elements are referred to as the
covariance parameters and collected in the vector $theta$.
%===========================================================================%

The concept of critiquing the model-data agreement applies in mixed models in the same way as in linear
fixed-effects models. In fact, because of the more complex model structure, you can argue that model and
data diagnostics are even more important. For example, you are not only concerned with capturing the
important variables in the model. You are also concerned with “distributing” them correctly between the
fixed and random components of the model. The mixed model structure presents unique and interesting
challenges that prompt us to reexamine the traditional ideas of influence and residual analysis.
%==========================================================================%
This paper presents the extension of traditional tools and statistical measures for influence and residual
analysis to the linear mixed model and demonstrates their implementation in the MIXED procedure (experimental
features in SAS 9.1). The remainder of this paper is organized as follows. The “Background” section
briefly discusses some mixed model estimation theory and the challenges to model diagnosis that result
from it.

%	 The diagnostics implemented in the MIXED procedure are discussed in the “Residual Diagnostics
%	in the MIXED Procedure” section (page 3) and the “Influence Diagnostics in the MIXED Procedure” section
%	(page 5). The syntax options and suboptions you use to request the various diagnostics are briefly sketched
%	in the “Syntax” section (page 9). The presentation concludes with an example.
%	
%	
%====================================================================================================================%