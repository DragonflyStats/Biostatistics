\documentclass[12pt, a4paper]{report}

\usepackage{epsfig}
\usepackage{subfigure}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{graphicx}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{subfiles}
\usepackage{framed}
\usepackage{subfiles}
\usepackage{amsthm, amsmath}
\usepackage{amsbsy}
\usepackage{framed}
\usepackage[usenames]{color}
\usepackage{listings}
\lstset{% general command to set parameter(s)
	basicstyle=\small, % print whole listing small
	keywordstyle=\color{red}\itshape,
	% underlined bold black keywords
	commentstyle=\color{blue}, % white comments
	stringstyle=\ttfamily, % typewriter type for strings
	showstringspaces=false,
	numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt, %
	frame=shadowbox,
	rulesepcolor=\color{black},
	,columns=fullflexible
} %
%\usepackage[dvips]{graphicx}
\usepackage{natbib}
\bibliographystyle{chicago}
\usepackage{vmargin}
% left top textwidth textheight headheight
% headsep footheight footskip
\setmargins{1.0cm}{0.75cm}{18.5 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}
%\voffset=-2.5cm
%\oddsidemargin=1cm
%\textwidth = 520pt

\renewcommand{\baselinestretch}{1.5}
\pagenumbering{arabic}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{ill}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{axiom}{Axiom}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{notation}{Notation}
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\renewcommand{\thenotation}{}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\title{Research notes: linear mixed effects models}
\author{ } \date{ }


\begin{document}
	\chapter{Bradley Blackwood}
	%----------------------------------------------------------------------------------------------------------------------%
	\section{Bartko's Bradley-Blackwood Test}
	This is a regression based
	approach that performs a simultaneous test for the equivalence of
	means and variances of the respective methods.We have identified
	this approach  to be examined to see if it can be used as a
	foundation for a test perform a test on
	means and variances individually.
	\begin{equation}
	D = (X_{1}-X_{2})
	\end{equation}
	\begin{equation}
	M = (X_{1} + X_{2}) /2
	\end{equation}
	The Bradley Blackwood Procedure fits D on M as follows:\\
	\begin{equation}
	D = \beta_{0} + \beta_{1}M
	\end{equation}
	\begin{itemize}
		\item The Bradley Blackwood test is a simultaneous test for bias and
		precision. They propose a regression approach which fits D on M,
		where D is the difference and average of a pair of results.
		\item Both beta values, the intercept and slope, are derived from the respective means and
		standard deviations of their respective data sets.
		\item We determine if the respective means and variances are equal if
		both beta values are simultaneously equal to zero. The Test is
		conducted using an F test, calculated from the results of a
		regression of D on M.
		\item We have identified this approach  to be examined to see if it can
		be used as a foundation for a test perform a test on means and
		variances individually.
		\item Russell et al have suggested this method be used in conjunction
		with a paired t-test , with estimates of slope and intercept.
	\end{itemize}
	%subsection{t-test}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%  Blackwood Bradley Model         %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	
	
\section*{Bartko's Discussion of BB}

Let $y = X_1 - X_2$ and $x= (X_1 - X_2)/2$.
The Bradley-Blackwood procedure fits $y$ on $x$, such that
\[ y = \beta_0 + \beta_1x \]

The slope and intercepte are given bu

\[beta_1 =  \frac{(\sigma^2_1 = \sigma^2_2)}{2\sigma^2_x}\]
%------------------------------------------------%


	\section{Bradley-Blackwood Test (Kevin Hayes Talk)}
	%--------------------------------------------------------------------%
	% KH - UW
	
	This work considers the problem of testing $\mu_1$ = $\mu_2$ and $\sigma^2_1 = \sigma^2_2$ using a random sample from a bivariate normal distribution with parameters $(\mu_1, \mu_2, \sigma^2_1, \sigma^2_2, \rho)$. 
	
	The new contribution is a decomposition of the Bradley-Blackwood test statistic (\textit{Bradley and Blackwood, 1989})for the simultaneous test of {$\mu_1$ = $\mu_2$; $\sigma^2_1 = \sigma^2_2$}  as a sum of two statistics. 
	
	One is equivalent to the Pitman-Morgan (\textit{Pitman, 1939; Morgan, 1939}) test statistic 
	for $\sigma^2_1 = \sigma^2_2$ and the other one is a new alternative to the standard paired-t test of $\mu_D = \mu_1 = \mu_2 = 0$. 
	
	Surprisingly, the classic Student paired-t test makes no assumptions about the equality (or otherwise) of the 
	variance parameters. 
	
	The power functions for these tests are quite easy to derive, and show that when $\sigma^2_1 = \sigma^2_2$, 
	the paired t-test has a slight advantage over the new alternative in terms of power, but when $\sigma^2_1 \neq \sigma^2_2$, the 
	new test has substantially higher power than the paired-t test.
	
	While Bradley and Blackwood provide a test on the joint hypothesis of equal means and equal variances their regression based approach does not separate these two issues.
	
	The rejection of the joint hypothesis may be 
	due to two groups with unequal means and unequal variances; unequal means and equal variances, or equal means and unequal variances. We propose an approach for resolving this (model selection) problem in a manner controlling the magnitudes of the relevant type I error probabilities.
	
	
	
	

	\section{Conclusions about Existing Methodologies}
	
	The Bland Altman methodology is well noted for its ease of use,
	and can be easily implemented with most software packages. Also it
	doesn't require the practitioner to have more than basic
	statistical training. The plot is quite informative about the
	variability of the differences over the range of measurements. For
	example, an inspection of the plot will indicate the 'fan effect'.
	They also can be used to detect the presence of an outlier.
	
	\citet{ludbrook97,ludbrook02} criticizes these plots on the
	basis that they presents no information on effect of constant bias
	or proportional bias. These plots are only practicable when both
	methods measure in the same units. Hence they are totally
	unsuitable for conversion problems. The limits of agreement are
	somewhat arbitrarily constructed. They may or may not be suitable
	for the data in question. It has been found that the limits given
	are too wide to be acceptable. There is no guidance on how to deal
	with outliers. Bland and Altman recognize effect they would have
	on the limits of agreeement, but offer no guidance on how to
	correct for those effects.
	
	There is no formal testing procedure provided. Rather, it is upon
	the practitioner opinion to judge the outcome of the methodology.
	
	
	
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%9 Appendix                  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	
	%
	%\section{Contention }
	%Several papers have commented that this approach is undermined
	%when the basic assumptions underlying linear regression are not
	%met, the regression equation, and consequently the estimations of
	%bias are undermined. Outliers are a source of error in regression
	%estimates.In method comparison studies, the X variable is a
	%precisely measured reference method. Cornbleet Gochman (1979)
	%argued that criterion may be regarded as the correct value. Other
	%papers dispute this.
	%
	
	
	
	%
	%
	%\section{A regression based approach based on Bland Altman Analysis}
	%Lu et al used such a technique in their comparison of DXA
	%scanners. They also used the Blackwood Bradley test. However it
	%was shown that, for particular comparisons,  agreement between
	%methods was indicated according to one test, but lack of agreement
	%was indicated by the other.
	
	
	
	\section*{Bartko's Ellipse}
	
	\[ \frac{x - \bar{x}}{\sigma^2_x} - \frac{2\rho(x - \bar{x})(y - \bar{y})}{\sigma_x \sigma_y} + \frac{y - \bar{y}}{\sigma^2_y} = \chi^2(2df_(1-\rho^2) \]
	%------------------------------------------------%
	
	

	\section{A regression based approach based on Bland Altman Analysis}
	Bland and Altman have stated that regression analysis offers insights into method comparison studies. Regression methods can determine the presence of bias, and the levels of constant bias and proportional bias thereof \cite{ludbrook97,ludbrook02}.
	While they are informative about inter-method bias, Regression methods offer the analyst no insights into the relative precision of both methods. These methods can be employed in conversion problems, however errors are
	attended.
	\emph{\textbf{Lu et al}} used such a technique in their comparison of DXA scanners. They also used the Blackwood Bradley test. However it was shown that, for particular comparisons, agreement between methods was indicated according to one test, but lack of agreement was indicated by the other.
	
	
	\section*{Remarks}
	\begin{itemize}
		\item Pearson's Correlation of (x,y) is the same as Pitman's correlation of sums and differences.
		
		\item Techniques for plotting an ellipse can be found in Douglas Altman's book.
	\end{itemize}
	%------------------------------------------------%
	\section{The MCR R pacakge - Regression Techniques for MCS}
	
	The \textbf{\textit{mcr}} packages provides a set of regression techniques to quantify the relation between two measurement methods.
	
	In particular, it address regression problems with errors in both variables, but without repeated measurements.
	The \textbf{\textit{mcr}} package follows the CLSI EP09-A3 recommendations for analytical
	method comparison and estimation of bias using patient samples.
	
	
	\textit{Methods featured in the \textbf{mcr} package}
	
	\begin{itemize}
		\item Deming Regression
		\item Weighted Deming Regression
		\item Passing-Bablock Regression
	\end{itemize}
	
	The \textit{creatinine} gives the blood and serum preoperative creatinine measurements in 110 heart surgery patients.
	
	\begin{framed}
		\begin{verbatim}
		library("mcr")
		data("creatinine", package="mcr")
		tail(creatinine)
		
		
		fit.lr <- mcreg(as.matrix(creatinine), method.reg="LinReg", na.rm=TRUE)
		fit.wlr <- mcreg(as.matrix(creatinine), method.reg="WLinReg", na.rm=TRUE)
		compareFit( fit.lr, fit.wlr )
		\end{verbatim}
	\end{framed}
	
	

	\section{KP}
	Most residual covariance structures are design for one
	within-subject factor. However two or more may be present. For
	such cases, an approppriate approach would be the residual
	covariance structure using Kronecker product of the underlying
	within-subject factor specific covariances structure.
	
	

\chapter{REGRESSION}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Regression
\section{Simple Linear Regression}

% 1.A. Use of SLR, description of SLR as Model I
% 1.B. Inappropriate for MCS
% 1.C  Calibration and Conversion problems

Simple linear regression is defined as such with the name `Model I regression' by Cornbleet Gochman (1979), in contrast to 'Model II regression'.

On account of the fact that one set of measurements are linearly related to another, one could surmise that Linear Regression is the most suitable approach to analyzing comparisons. This approach is unsuitable on two counts. Firstly one of the assumptions of Regression analysis is that the independent variable values are without error. In method comparison studies one must assume the opposite; that there is error present in the measurements. Secondly a regression of X on Y would yield and entirely different result from Y on X.


Simple linear regression calculates a line of best fit for two
sets of data, n which the independent variable, X, is measured without error, with y as the dependent variable.  

SLR (Model I) regression is considered by many \citet{BA83,CornCoch,ludbrook97} to be wholly unsuitable for
method comparison studies, although recommended for use in calibration studies [Corncoch]. Even in the case where one
method is a gold standard , it is disputed as to whether it is a valid approach. Model II regression is more suitable for method comparison studies, but it is more difficult to execute. Both Model I and II regression models are unduly influenced by outliers. Regression Models can not be used to analyze repeated measurements

\subsubsection{Regression Analysis}
Another inappropriate approach is the regressing one set of measurements against the other. According to this methodology the measurement methods could considered equivalent if the confidence interval for
the regression coefficient included $1$. Analysts sometimes use least squares (referred to by Ludbrook as Model I) regression analysis to calibrate one method of measurement against another. In this technique, the sum of the squares of the vertical deviations of y values from the line is minimized. This approach is invalid, because both y and x values are attended by random error.


\subsubsection{The Identity Plot} This is a simple graphical approach, advocated by \citet{BA86}, that yields a cursory examination of how well the measurement methods agree. In the case of good agreement, the co-variates of the plot accord closely with the $X=Y$ line.

\subsubsection{Advantages of Regression Approaches for MCS}
\begin{itemize}
	\item These methods can be employed in conversion problems.
	\item Bland and Altman have stated that regression analysis offers insights into MCS problems.
\end{itemize}
\subsubsection{Disadvantages}
\begin{itemize}
	\item Regression methods are uninformative about the variability of the differences.
\end{itemize}

\begin{itemize}\item
	Regression methods can determine the presence of bias, and the levels of constant bias and proportional bias thereof \cite{ludbrook97,ludbrook02}.
\end{itemize}

%------------------------------------------------------------------------%

\section{Constant and Proportional Bias}

Linear Regression is a commonly used technique for comparing paired assays. The Intercept and Slope can provide estimates for the constant bias and proportional bias occurring between both methods. If the basic assumptions underlying linear regression are not met, the regression equation, and consequently the estimations
of bias are undermined. Outliers are a source of error in regression estimates.

Constant or proportional bias in method comparison studies using linear regression can be detected by an individual test on the intercept or the slope of the line regressed from the results of the two methods to be compared.






\section*{Pitman's Test on Correlated variances}
%Bartko Page 741
\begin{description}
	\item[$H_0$] : $\sigma^2_1 = \sigma^2_2$
	\item[$H_0$] : $\sigma^2_1 = \sigma^2_2$
\end{description}


Pitman's test is identical to the slope equal to zero in the regression of $y$ on $x$.

%------------------------------------------------%



\section{Regression Approaches to MCS}
%============================================ %
- This section of the paper will discuss several regression-based techniques for method comparison study, 
such as *Deming Regression*. 
- This section will highlight several useful characteristics of the data that 
regression based techniques may highlight, and also the limitations of these techniques.

\subsection*{ Deming Regression}
Whereas the OLS method assumes that only the Y measurements are associated with 
random measurement errors, the Deming method takes measurement errors for both methods of measurement into account.




% <magari>
The presence of bias may impair agreement between the two analytical methods.

types of bias

\begin{itemize}
	\item constant bais
	\item proportionl bias
\end{itemize}

\subsection{Passing and Bablok (1983) }
Passing \& Bablok have described a linear regression model that are without the usual assumptions regarding the distribution of
the samples and the measurement errors. The result does not depend on the assignment of the methods (or instruments) to X and Y. The slope and intercept  are calculated with their 95\% confidence interval.Hypothesis tests on the slope and intercept maybe then
carried out.\\

If the hypothesis of the intercept is rejected, then it is concluded that it is significant different from $0$ and both
raters differ at least by a constant amount.
	\\
If the hypothesis of the slope is rejected, then it is concluded that the slope is significant different from $1$ and there is at
least a proportional difference between the two raters.
%-----------------------------------------------------------%
\subsection{Passing-Bablok Regression}

% www.medcalc.be
% MCR package

Passing \& Bablok (1983) have described a linear regression procedure with no special assumptions regarding the distribution of the samples and the measurement errors. The result does not depend on the assignment of the methods (or instruments) to X and Y. The slope B and intercept A are calculated with their 95\% confidence interval. These confidence intervals are used to determine whether there is only a chance difference between B and 1 and between A and 0.


\subsection{Passing and Bablok ( 1983) }
Passing \& Bablok have described a linear regression model that are without the usual assumptions regarding the distribution of
the samples and the measurement errors. The result does not depend on the assignment of the methods (or instruments) to X and Y. The
slope and intercept  are calculated with their 95\% confidence interval.Hypothesis tests on the slope and intercept maybe then
carried out.
\\
If the hypothesis of the intercept is rejected, then it is concluded that it is significant different from $0$ and both
raters differ at least by a constant amount.
\\
If the hypothesis of the slope is rejected, then it is concluded that the slope is significant different from $1$ and there is at
least a proportional difference between the two raters.

\subsection{Passing and bablok}
proposed an linear regression procedure with no special assumptions regarding the distribution of the data.
This non parametric method is based on ranking the observatons so it is computationally intensive.
The result is independent of the assignment of the reference method (X) and the reference method (Y).

" a new biometric method procedure for testing the equality of measurements from two different analytical methods"
J Clin. Chem Clin.BioChem. [21], 709-720 (1983)
%-----------------------------------------------------------%
\subsection{Implementation with \texttt{R}}

%--------------%
\begin{framed}
	\begin{verbatim}
	
	library(mcr)
	
	\end{verbatim}
\end{framed}
%--------------%
% <magari>
The presence of bias may impair agreement between the two analytical methods.

types of bias

\begin{itemize}
	\item constant bais
	\item proportionl bias
\end{itemize}
%-------------------------------------------------------------------------------%






\addcontentsline{toc}{section}{Bibliography}

%--------------------------------------------------------------------------------------%

\bibliographystyle{chicago}
\bibliography{DB-txfrbib}
\end{document}
