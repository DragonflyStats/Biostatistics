\documentclass[Main.tex]{subfiles}
\begin{document}
	\section{Outliers and Leverage}
	
	
	
	The question of whether or not a point should be considered an outlier must also be addressed. An outlier is an observation whose true value is unusual given its value on the predictor variables. The leverage of an observation is a further consideration. Leverage describes an observation with an extreme value on a predictor variable is a point with high leverage. High leverage points can have a great amount of effect on the estimate of regression coefficients.
	% - Leverage is a measure of how far an independent variable deviates from its mean.
	
	Influence can be thought of as the product of leverage and outlierness. An observation is said to be influential if removing the observation substantially changes the estimate of the regression coefficients. The \texttt{R} programming language has a variety of methods used to study each of the aspects for a linear model. While linear models and GLMS can be studied with a wide range of well-established diagnostic technqiues, the choice of methodology is much more restricted for the case of LMEs.
	
	%---------------------------------------------------------------------------%
	%\newpage
	%\section{Residual diagnostics} %1.3
	For classical linear models, residual diagnostics are typically conducted using a plot of the observed residuals and the predicted values. A visual inspection for the presence of trends inform the analyst on the validity of distributional assumptions, and to detect outliers and influential observations.
\subsection{Leverage}
Leverage can be defined through the projection matrix that results from a transformation of the model with the inverse of the Cholesky decomposition of $\boldsymbol{V}$, or an oblique projector:	$\boldsymbol{Y} = \boldsymbol{H}\boldsymbol{\hat{Y}}$.

While $H$ is idempotent, it is generally not symmetric and thus not a projection matrix in the narrow sense.
\[ h_{ii} = x^{\prime}_{i}(X^{\prime}X)^{-1}x_{i} \]
The trace of $\boldsymbol{H}$ equals the rank of $\boldsymbol{X}$.
If $V_{ij}$ denotes the element in row $i$, column $j$ of $\boldsymbol{V}^{-1}$, then for a model containing only an intercept the diagonal elements of $\boldsymbol{H}$.

\[ h_{ii} = \frac{\sum v_{ij}}{\sum \sum v_{ij}} \]


%http://www.ime.usp.br/~jmsinger/MAE0610/Mixedmodelresiduals.pdf

\subsection*{Nobre Singer :  Mixed Model Residuals }

%--------------------------------------------------------------%
% Slides
Usually one assumes
\begin{itemize}
\item $b_i \sim N_q(0, G) i = 1, ..., m$
\item $e_i  \sim N_{n_i} (0, \sigma_i)$
\item $b_i$ and $e_i$ independent
\item G and $\sigma_i$ are $(q \times q)$ and $(n_i \times n_i)$ positive deÔ¨Ånite matrices with
elements expressed as functions of a vector of covariance parameters $\theta$ not functionally related to $\beta$
\item If $\sigma_i = I_{n_i} \sigma^2$: homoskedastic conditional independence model
\end{itemize}
%-------------------------------------------------------------%

%Page 1064
\[  \left[ \begin{array}{c} \boldsymbol{b} \\ \boldsymbol{e} \end{array}\right] \sim \mathcal{N}_{qm+n} \] 

%-------------------------------------------------------------%
%Page 1065 Top

\[ \boldsymbol{Q} = \boldsymbol{V}^{-1} - \boldsymbol{V}^{-1}\boldsymbol{X} ( \boldsymbol{X}^{T} \boldsymbol{V}^{-1} \boldsymbol{X})^{-1} \]

%Papers  : Harville, Robinson, Banerjee

Sensitivity and residual analysis of the underlying assumptions constitute important tools for evaluating the fit of any model to given data.


%-------------------------------------------------------------%
%Page 1065 Bottom
\subsection*{Generalized Leverage}

%-------------------------------------------------------------%
%Page 1068



%%-----------------------------------------------------------------%
%
%Comparison between the two generalized random component leverage matrics
%
%%-------------------------------------------------------------%
%Section 4
%Data Analysis

\end{document}
