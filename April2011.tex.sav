\documentclass[12pt, a4paper]{article}
\usepackage{epsfig}
\usepackage{subfigure}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{amsthm, amsmath}
%\usepackage[dvips]{graphicx}
\usepackage{natbib}
\bibliographystyle{chicago}
\usepackage{vmargin}
% left top textwidth textheight headheight
% headsep footheight footskip
\setmargins{3.0cm}{2.5cm}{15.5 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}
\renewcommand{\baselinestretch}{1.5}
\pagenumbering{arabic}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{ill}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{axiom}{Axiom}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{notation}{Notation}
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\renewcommand{\thenotation}{}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\title{Research notes: linear mixed effects models}
\author{ } \date{ }


\begin{document}
\author{Kevin O'Brien}
\title{Updating techniques for LME models}

\addcontentsline{toc}{section}{Bibliography}

%----------------------------------------------------------------------------------------%
\newpage
%\chapter{Limits of Agreement}

\section{Modelling Agreement with LME Models}



\citet{roy} considers the matter of comparing the agreement of two methods using a linear mixed effects (LME) model.
%A full discussion of the LME model can be found in \citet{pb}.
An LME model used to describe replicate measurements made by two methods on a number of individuals is presented.

Let $y_{mir} $ be the $r-$th replicate measurement of the $i$th
item (e.g. patient) by the $m-$th method. It is assumed that only
two methods to be compared, hence $m=\{1,2\}$.
% The true value of the measurement of the $i-$th item is a fixed effect, denoted $\beta_{i}$.
The fixed effect for each method, that gives rise to inter-method
bias, is denoted $\beta_{m}$.  The inter-method bias is therefore
$\beta_{1} - \beta_{2}$. Additionally there is an intercept
term$\beta_0$, necessarily the true mean of the population of
items, rather than the true mean of any particular item. This
point will have relevance when comparing different models later
on.(Remark: Carstensen's model does use a term to describe the
true value of an item).

\begin{equation}
y_{mir} = \beta_{0} + \beta_{m} + b_{mi} + \epsilon_{mir}
\end{equation}

The random effect for item $i$ , which is associated with method
$m$, is denoted $b_{mi}$. The residual is denoted
$\epsilon_{mir}$. The variances of both shall be considered in due
course.

\subsection{Response Vector}
\noindent Consider the response vector $\boldsymbol{y}_{i}$ comprises the $2n_i$ observations of the item, as measured by two methods, taking $n_i$ measurements each. (For expository purposes, we will say $n_i = 3$. Hence a $6 \times 1$ random vector corresponding to the $i$th subject.)

Each response on item $i$ is stacked into a response vector.
\begin{equation}
\boldsymbol{y}_{i} = (y_{1i1},y_{2i1},y_{1i2},\ldots,y_{mir},\ldots,y_{1in_{i}},y_{2in_{i}}) \prime
\end{equation}

To formulate a model for the response vector $\boldsymbol{y}_{i}$, the fixed effects for both methods are given as $\beta_1$ and $\beta_2$ , in addition to the true value effect $\beta_i$, respectively, while the random effect terms are given as $b_{1}$ and $b_{2}$. Two matrices of indicator variables (later referred to as $\boldsymbol{X}_i$ and $\boldsymbol{Z}_i$) enable the correct effects for each response.
Thus, for two methods and each with three replicates, the response vector would be formulated as

\begin{equation}
\boldsymbol{y}_{i} = \left(
                         \begin{array}{ccc}
                           1 & 1 & 0 \\
                           1 & 0 & 1 \\
                           1 & 1 & 0 \\
                           1 & 0 & 1 \\
                           1 & 1 & 0 \\
                           1 & 0 & 1 \\
                         \end{array}
                       \right)\left(
                                \begin{array}{c}         \beta_0 \\ \beta_1 \\ \beta_2 \\
                                \end{array}
                              \right)
                        +  \left(
                         \begin{array}{cc}
                           1 & 0 \\
                           0 & 1 \\
                           1 & 0 \\
                           0 & 1 \\
                           1 & 0 \\
                           0 & 1 \\
                         \end{array}
                       \right)\left(
                                \begin{array}{c}
                                  b_{1i} \\   b_{2i} \\
                                \end{array}
                              \right)
                              +
                              \left(
                                          \begin{array}{c}
                                            \epsilon_{1i1} \\
                                            \epsilon_{2i1} \\
                                            \epsilon_{1i2} \\
                                            \epsilon_{2i2} \\
                                            \epsilon_{1i3} \\
                                            \epsilon_{2i3} \\
                                          \end{array}
                                        \right)
                              \end{equation}

This model can be conveniently presented in matrix form as follows;
\begin{center}
\begin{equation}
 \boldsymbol{y_{i}} = \boldsymbol{X_{i}\beta}
+ \boldsymbol{Z_{i}b_{i}} + \boldsymbol{\epsilon_{i}}, \qquad i=1,\dots,I \end{equation}
\end{center}

It is assumed that $\boldsymbol{b}_i \sim N(0,\boldsymbol{G})$,
$\boldsymbol{\epsilon}_i$ is a matrix of random errors distributed as $N(0,\boldsymbol{R}_i)$ and
that the random effects and residuals are independent of each other. Assumptions made on the structures of $\boldsymbol{G}$ and $\boldsymbol{R}_i$ will be discussed in due course.

\newpage
It is important to note the following characteristics of this model.
\begin{itemize}
\item Let the number of replicate measurements on each item $i$ for both methods be $n_i$, hence $2 \times n_i$ responses. However, it is assumed that there may be a different number of replicates made for different items. Let the maximum number of replicates be $p$. An item will have up to $2p$ measurements, i.e. $\max(n_{i}) = 2p$.
\item $\boldsymbol{y}_i$ is the $2n_i \times 1$ response vector for measurements on the $i-$th item.
\item $\boldsymbol{X}_i$ is the $2n_i \times  3$ model matrix for the fixed effects for observations on item $i$.
\item $\boldsymbol{\beta}$ is the $3 \times  1$ vector of fixed-effect coefficients, one for the true value for item $i$, and one effect each for both methods.
\item Later on $\boldsymbol{X}_i$ will be reduced to a $2 \times 1$ matrix, to allow estimation of terms. This is due to a shortage of rank. The fixed effects vector will have to be modified accordingly.
\item $\boldsymbol{Z}_i$ is the $2n_i \times  2$ model matrix for the random effects for measurement methods on item $i$.
\item $\boldsymbol{b}_i$ is the $2 \times  1$ vector of random-effect coefficients on item $i$, one for each method.
\item $\boldsymbol{\epsilon}$  is the $2n_i \times  1$ vector of residuals for measurements on item $i$.
\item $\boldsymbol{G}$ is the $2 \times  2$ covariance matrix for the random effects.
\item $\boldsymbol{R}_i$ is the $2n_i \times  2n_i$ covariance matrix for the residuals on item $i$.
\item The expected value is given as $\mbox{E}(\boldsymbol{y}_i) = \boldsymbol{X}_i\boldsymbol{\beta}.$ \citep{hamlett}
\item The variance of the response vector is given by $\mbox{Var}(\boldsymbol{y}_i)  = \boldsymbol{Z}_i \boldsymbol{G} \boldsymbol{Z}_i^{\prime} + \boldsymbol{R}_i$ \citep{hamlett}.
\end{itemize}
\newpage



\newpage
\section{Variance Matrices}

$\boldsymbol{G}$ is the variance covariance matrix for the random effects.
i.e. between-item sources of variation. The between-item variance covariance matrix $\boldsymbol{G}$ is constructed as follows:

\[\boldsymbol{G} =\left(
            \begin{array}{cc}
              g^2_1  & g_{12} \\
              g_{12} & g^2_2 \\
            \end{array}
          \right) \]
It is important to note that no special assumptions about the structure of $\boldsymbol{G}$ are made. An example of such an assumption would be that $\boldsymbol{G}$ is the product of a scalar value and the identity matrix.

$\boldsymbol{R}_{i}$ is the variance covariance matrix for the residuals, i.e. the within-item sources of variation between both methods. Computational analysis of linear mixed effects models allow for the explicit analysis of each.

\citet{hamlett} shows that $\boldsymbol{R}_{i}$  can be expressed as $\boldsymbol{R}_{i} = \boldsymbol{I}_{n_{i}} \otimes \boldsymbol{\Sigma}$. The partial within-subject variance–covariance matrix of two methods at any replicate is denoted $\boldsymbol{\Sigma}$, where $\sigma^2_{1}$ and $\sigma^2_{2}$ are the within-subject variances of the respective methods, and $\sigma_{12}$ is the within-subject covariance between the two methods. It is assumed that the within-subject variance–covariance matrix $\boldsymbol{\Sigma}$ is the same for all replications. Again it is important to note that no special assumptions are made about the structure of the matrix.

\begin{equation}
\boldsymbol{\Sigma} = \left( \begin{array}{cc}
  \sigma^2_{1} & \sigma_{12} \\
  \sigma_{12} & \sigma^2_{2} \\
\end{array}\right)
\end{equation}

\subsection{Coefficient of Repeatability}
The coefficient of repeatability is a measure of how well a
measurement method agrees with itself over replicate measurements
\citep{BA99}. Once the within item variability is known, the
computation of the coefficients of repeatability for both methods
is straightforward.

\subsection{Overall variability} The overall variability between
the two methods is the sum of between-item variability
$\boldsymbol{G}$ and within-item variability
$\boldsymbol{\Sigma}$. \citet{roy} denotes the overall variability
as ${\mbox{Block - }\boldsymbol \Omega_{i}}$. The overall
variation for methods $1$ and $2$ are given by



\begin{center}
\[\left(\begin{array}{cc}
                \omega^2_1  & \omega_{12} \\
              \omega_{12} & \omega^2_2 \\
            \end{array}  \right)
            =  \left(
            \begin{array}{cc}
              g^2_1  & g_{12} \\
              g_{12} & g^2_2 \\
            \end{array} \right)+
            \left(
            \begin{array}{cc}
              \sigma^2_1  & \sigma_{12} \\
              \sigma_{12} & \sigma^2_2 \\
            \end{array}\right)
\]
\end{center}
The computation of the limits of agreement require that the variance of the difference of measurements. This variance is easily computable from the estimate of the ${\mbox{Block - }\boldsymbol \Omega_{i}}$ matrix. Lack of agreement can arise if there is a disagreement in overall variabilities. This may be due to due to the disagreement in either between-item
variabilities or within-item variabilities, or both. \citet{roy} allows for a formal test of each.

\subsection{Hypothesis Testing}
The formulation presented above usefully facilitates a series of
significance tests that advise as to how well the two methods
agree.


\newpage
\section{Carstensen's Limits of agreement}
\citet{bxc2008} presents a methodology to compute the limits of
agreement based on LME models. Importantly, Carstensen's underlying model differs from Roy's model in some key respects, and therefore a prior discussion of Carstensen's model is required.

\subsection{Carstensen's Model}

\citet{BXC2004} presents a model to describe the relationship between a value of measurement and its
real value. The non-replicate case is considered first, as it is the context of the Bland Altman plots. This model assumes that inter-method bias is the only difference between the two methods.

A measurement $y_{mi}$ by method $m$ on individual $i$ is formulated as follows;
\begin{equation}
y_{mi}  = \alpha_{m} + \mu_{i} + e_{mi} \qquad  e_{mi} \sim
\mathcal{N}(0,\sigma^{2}_{m})

\end{equation}
The differences are expressed as $d_{i} = y_{1i} - y_{2i}$. For the replicate case, an interaction term $c$ is added to the model, with an associated variance component. All the random effects are assumed independent, and that all replicate measurements are assumed to be exchangeable within each method.

\begin{equation}
y_{mir}  = \alpha_{m} + \mu_{i} + c_{mi} + e_{mir}, \qquad  e_{mi}
\sim \mathcal{N}(0,\sigma^{2}_{m}), \quad c_{mi} \sim \mathcal{N}(0,\tau^{2}_{m}).
\end{equation}
%----
 
Of particular importance is terms of the model, a true value for individual $i$ ($\mu_{i}$). This term is absent from Roy's model, whose fixed effect elements comprise of an intercept term and fixed effect terms for both methods.

 
\subsection{Assumptions on Variability}

Another important difference is that Carstensen's
model requires that particular assumptions be applied,
specifically that the off-diagonal elements of the between-item
and within-item variability matrices are zero. By extension the
overall variability off diagonal elements are also zero.

Also, implementation requires that the between-item variances are
estimated as the same value: $g^2_1 = g^2_2 = g^2$. Necessarily
Carstensen's method does not allow for a formal test of the
between-item variability.

\[\left(\begin{array}{cc}
                \omega^1_2  & 0 \\
              0 & \omega^2_2 \\
            \end{array}  \right)
            =  \left(
            \begin{array}{cc}
              g^2  & 0 \\
              0 & g^2 \\
            \end{array} \right)+
            \left(
            \begin{array}{cc}
              \sigma^2_1  & 0 \\
              0 & \sigma^2_2 \\
            \end{array}\right)
\]

In cases where the off-diagonal terms in the overall variability
matrix are close to zero, the limits of agreement due to
\citet{bxc2008} are very similar to the limits of agreement that
follow from the general model.

\newpage
\bibliography{DB-txfrbib}
\end{document}
