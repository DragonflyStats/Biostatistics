\documentclass[12pt, a4paper]{article}
\usepackage{natbib}
\usepackage{vmargin}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{subfigure}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{subfiles}
\usepackage{subfigure}
\usepackage{framed}
\usepackage{subfiles}
\usepackage{amsbsy}
\usepackage{amsthm, amsmath}
%\usepackage[dvips]{graphicx}
\bibliographystyle{chicago}
\renewcommand{\baselinestretch}{1.1}

% left top textwidth textheight headheight % headsep footheight footskip
\setmargins{3.0cm}{2.5cm}{15.5 cm}{23.5cm}{0.25cm}{0cm}{0.5cm}{0.5cm}

\pagenumbering{arabic}



\begin{document}
\section{Introduction}%1.1
In classical linear models model diagnostics have been become a required part of any statistical analysis, and the methods are commonly available in statistical packages and standard textbooks on applied regression. However it has been noted by several papers that model diagnostics do not often accompany LME model analyses.
Model diagnostic techniques determine whether or not the distributional assumptions are satisfied, and to assess the influence of unusual observations.

\subsection{What is Influence} %1.1.5


Broadly defined, influence is understood as the ability of a single or multiple data points, through their presence or absence in the data, to alter important aspects of the analysis, yield qualitatively different inferences, or violate assumptions of the statistical model. The goal of influence analysis is not primarily to mark data
points for deletion so that a better model fit can be achieved for the reduced data, although this might be a result of influence analysis \citep{schabenberger}.

%---------------------------------------------------------------------------%
%---------------------------------------------------------------------------%
\newpage
\subsection{Influence Diagnostics: Basic Idea and Statistics} %1.1.2
%http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mixed_sect024.htm

The general idea of quantifying the influence of one or more observations relies on computing parameter estimates based on all data points, removing the cases in question from the data, refitting the model, and computing statistics based on the change between full-data and reduced-data estimation. 




\subsection{Influence Statistics for LME models} %1.1.4
Influence statistics can be coarsely grouped by the aspect of estimation that is their primary target:
\begin{itemize}
	\item overall measures compare changes in objective functions: (restricted) likelihood distance (Cook and Weisberg 1982, Ch. 5.2)
	\item influence on parameter estimates: Cook's  (Cook 1977, 1979), MDFFITS (Belsley, Kuh, and Welsch 1980, p. 32)
	\item influence on precision of estimates: CovRatio and CovTrace
	\item influence on fitted and predicted values: PRESS residual, PRESS statistic (Allen 1974), DFFITS (Belsley, Kuh, and Welsch 1980, p. 15)
	\item outlier properties: internally and externally studentized residuals, leverage
\end{itemize}
%---------------------------------------------------------------------------%





\subsection{Extension of techniques to LME Models} %1.2


Model diagnostic techniques, well established for classical models, have since been adapted for use with linear mixed effects models.Diagnostic techniques for LME models are inevitably more difficult to implement, due to the increased complexity.


Beckman, Nachtsheim and Cook (1987) \citet{Beckman} applied the \index{local influence}local influence method of Cook (1986) to the analysis of the linear mixed model.


While the concept of influence analysis is straightforward, implementation in mixed models is more complex. Update formulae for fixed effects models are available only when the covariance parameters are assumed to be known.


If the global measure suggests that the points in $U$ are influential, the nature of that influence should be determined. In particular, the points in $U$ can affect the following


\begin{itemize}
	\item the estimates of fixed effects,
	\item the estimates of the precision of the fixed effects,
	\item the estimates of the covariance parameters,
	\item the estimates of the precision of the covariance parameters,
	\item fitted and predicted values.
\end{itemize}



\section{What is Influence} %1.2

Broadly defined, ``influence"is understood as the ability of a single or multiple data points, through their presence
or absence in the data, to alter important aspects of the analysis, yield qualitatively different inferences, or
violate assumptions of the statistical model. The goal of influence analysis is not primarily to mark data
points for deletion so that a better model fit can be achieved for the reduced data, although this might be a
result of influence analysis \citep{schabenberger}.

Influence is defined as the `ability of a single or multiple data points, through their presence or absence
\section{Analysis of  Influence}

\subsection{Influence Analysis for LME Models} %1.1.3
The linear mixed effects model is a useful methodology for fitting a wide range of models. However, linear mixed effects models are known to be sensitive to outliers. \citet{CPJ} advises that identification of outliers is necessary before conclusions may be drawn from the fitted model.

Standard statistical packages concentrate on calculating and testing parameter estimates without considering the diagnostics of the model.The assessment of the effects of perturbations in data, on the outcome of the analysis, is known as statistical influence analysis. Influence analysis examines the robustness of the model. Influence analysis methodologies have been used extensively in classical linear models, and provided the basis for methodologies for use with LME models.
Computationally inexpensive diagnostics tools have been developed to examine the issue of influence \citep{Zewotir}.
%Studentized residuals, error contrast matrices and the inverse of the response variance covariance matrix are regular components of these tools.


%-------------------------------------------------------------------------------------------------------------------------------------%
\subsection{Quantifying Influence}  %1.2.1

The basic procedure for quantifying influence is simple as follows:
\begin{itemize}
	\item Fit the model to the data and obtain estimates of all parameters.
	\item Remove one or more data points from the analysis and compute updated estimates of model parameters.
	\item Based on full- and reduced-data estimates, contrast quantities of interest to determine how the absence
	of the observations changes the analysis.
\end{itemize}

\subsection{Influence Diagnostics: Basic Idea and Statistics} %1.1.2
%http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mixed_sect024.htm

The general idea of quantifying the influence of one or more observations relies on computing parameter estimates based on all data points, removing the cases in question from the data, refitting the model, and computing statistics based on the change between full-data and reduced-data estimation. 



\subsection{Influence Analysis for LME Models} %1.1.3
The linear mixed effects model is a useful methodology for fitting a wide range of models. However, linear mixed effects models are known to be sensitive to outliers. \citet{CPJ} advises that identification of outliers is necessary before conclusions may be drawn from the fitted model.

Standard statistical packages concentrate on calculating and testing parameter estimates without considering the diagnostics of the model.The assessment of the effects of perturbations in data, on the outcome of the analysis, is known as statistical influence analysis. Influence analysis examines the robustness of the model. Influence analysis methodologies have been used extensively in classical linear models, and provided the basis for methodologies for use with LME models.
Computationally inexpensive diagnostics tools have been developed to examine the issue of influence \citep{Zewotir}.
Studentized residuals, error contrast matrices and the inverse of the response variance covariance matrix are regular components of these tools.


\section{Influence analysis} %1.7

Likelihood based estimation methods, such as ML and REML, are sensitive to unusual observations. Influence diagnostics are formal techniques that assess the influence of observations on parameter estimates for $\beta$ and $\theta$. A common technique is to refit the model with an observation or group of observations omitted.

\citet{west} examines a group of methods that examine various aspects of influence diagnostics for LME models.
For overall influence, the most common approaches are the `likelihood distance' and the `restricted likelihood distance'.

\subsection{Cook's 1986 paper on Local Influence}%1.7.1
Cook 1986 introduced methods for local influence assessment. These methods provide a powerful tool for examining perturbations in the assumption of a model, particularly the effects of local perturbations of parameters of observations.

The local-influence approach to influence assessment is quitedifferent from the case deletion approach, comparisons are of
interest.



\subsection{Overall Influence}
An overall influence statistic measures the change in the objective function being minimized. For example, in
OLS regression, the residual sums of squares serves that purpose. In linear mixed models fit by
\index{maximum likelihood} maximum likelihood (ML) or \index{restricted maximum likelihood} restricted maximum likelihood (REML), an overall influence measure is the \index{likelihood distance} likelihood distance [Cook and Weisberg ].


\section{Iterative and non-iterative influence analysis} %1.13
\citet{schabenberger} highlights some of the issue regarding implementing mixed model diagnostics.

A measure of total influence requires updates of all model parameters.

however, this doesnt increase the procedures execution time by the same degree.
\subsection{Iterative Influence Analysis}

%----schabenberger page 8
For linear models, the implementation of influence analysis is straightforward.
However, for LME models, the process is more complex. Update formulas for the fixed effects are available only when the covariance parameters are assumed to be known. A measure of total influence requires updates of all model parameters.
This can only be achieved in general is by omitting observations, then refitting the model.

\citet{schabenberger} describes the choice between \index{iterative influence analysis} iterative influence analysis and \index{non-iterative influence analysis} non-iterative influence analysis.

%---------------------------------------------------------------------------%
\newpage
\section{Influence analysis} %1.8

Likelihood based estimation methods, such as ML and REML, are sensitive to unusual observations. Influence diagnostics are formal techniques that assess the influence of observations on parameter estimates for $\beta$ and $\theta$. A common technique is to refit the model with an observation or group of observations omitted.

\citet{west} examines a group of methods that examine various aspects of influence diagnostics for LME models.
For overall influence, the most common approaches are the `likelihood distance' and the `restricted likelihood distance'.

\subsection{Cook's 1986 paper on Local Influence}%1.8.1
Cook 1986 introduced methods for local influence assessment. These methods provide a powerful tool for examining perturbations in the assumption of a model, particularly the effects of local perturbations of parameters of observations.

The local-influence approach to influence assessment is quitedifferent from the case deletion approach, comparisons are of
interest.



\subsection{Overall Influence}
An overall influence statistic measures the change in the objective function being minimized. For example, in
OLS regression, the residual sums of squares serves that purpose. In linear mixed models fit by
\index{maximum likelihood} maximum likelihood (ML) or \index{restricted maximum likelihood} restricted maximum likelihood (REML), an overall influence measure is the \index{likelihood distance} likelihood distance [Cook and Weisberg ].






%-------%
\subsection{Quantifying Influence}  %1.1.6


The basic procedure for quantifying influence is simple as follows:


\begin{itemize}
	\item Fit the model to the data and obtain estimates of all parameters.
	\item Remove one or more data points from the analysis and compute updated estimates of model parameters.
	\item Based on full- and reduced-data estimates, contrast quantities of interest to determine how the absence of the observations changes the analysis.
\end{itemize}


\citet{cook86} introduces powerful tools for local-influence assessment and examining perturbations in the assumptions of a model. In particular the effect of local perturbations of parameters or observations are examined.







\subsection{Model Data Agreement}
Schabenberger(20XX) describes the examination of model-data agreement as comprising several elements; \begin{itemize}
	\item residual analysis, 
	\item goodness of fit, 
	\item collinearity diagnostics
	\item influence analysis.
\end{itemize}



\section*{Influence Diagnostics : Basic Idea and Statistics}

The general idea of quantifying the influence of one or more observations relies on computing parameter estimates based on all data points, removing the cases in question from the data, refitting the model, and computing statistics based on the change between full-data and reduced-data estimation. 

Influence statistics can be coarsely grouped by the aspect of estimation that is their primary target:
\begin{itemize}
	\item overall measures compare changes in objective functions: (restricted) likelihood distance (Cook and Weisberg 1982, Ch. 5.2)
	\item influence on parameter estimates: Cook’s  (Cook 1977, 1979), MDFFITS (Belsley, Kuh, and Welsch 1980, p. 32)
	\item influence on precision of estimates: CovRatio and CovTrace
	\item influence on fitted and predicted values: PRESS residual, PRESS statistic (Allen 1974), DFFITS (Belsley, Kuh, and Welsch 1980, p. 15)
	\item outlier properties: internally and externally studentized residuals, leverage
\end{itemize}
For linear models for uncorrelated data, it is not necessary to refit the model after removing a data point in order to measure the impact of an observation on the model. The change in fixed effect estimates, residuals, residual sums of squares, and the variance-covariance matrix of the fixed effects can be computed based on the fit to the full data alone. By contrast, in mixed models several important complications arise. Data points can affect not only the fixed effects but also the covariance parameter estimates on which the fixed-effects estimates depend. 

Furthermore, closed-form expressions for computing the change in important model quantities might not be available.
This section provides background material for the various influence diagnostics available with the MIXED procedure. See the section Mixed Models Theory for relevant expressions and definitions. The parameter vector  denotes all unknown parameters in the  and  matrix.
The observations whose influence is being ascertained are represented by the set  and referred to simply as "the observations in ." The estimate of a parameter vector, such as , obtained from all observations except those in the set  is denoted . In case of a matrix , the notation  represents the matrix with the rows in  removed; these rows are collected in . If  is symmetric, then notation  implies removal of rows and columns. The vector  comprises the responses of the data points being removed, and  is the variance-covariance matrix of the remaining observations. When , lowercase notation emphasizes that single points are removed, such as .

	
	%-----------------------------------------------------------------%
	\section*{Diagnostic Methods for OLS models}
	% Cook's Distance for OLS models
	% http://www.amstat.org/meetings/jsm/2012/onlineprogram/AbstractDetails.cfm?abstractid=305411
	Influence diagnostics are formal techniques allowing for the identification of observations that exert substantial 
	influence on the estimates of fixed effects and variance covariance parameters. 
	
	The idea of influence diagnostics for a given observation is to quantify the effect of omission of this observation 
	from the data on the results of the model fit. To this aim, the concept of likelihood displacement is used. 
	
	%---------------------------------------------------------------%
	% We have developed a function in R, which allows performing influence diagnostics for linear mixed effects models 
	% fitted using the lme() function from the nlme package. 
	% The use of the new function is illustrated using data from a randomized clinical trial.
	


\section{Iterative and non-iterative influence analysis} %1.14
\citet{schabenberger} highlights some of the issue regarding implementing mixed model diagnostics.

A measure of total influence requires updates of all model parameters.

however, this doesnt increase the procedures execution time by the same degree.
\subsection{Iterative Influence Analysis}

%----schabenberger page 8
For linear models, the implementation of influence analysis is straightforward.
However, for LME models, the process is more complex. Update formulas for the fixed effects are available only when the covariance parameters are assumed to be known. A measure of total influence requires updates of all model parameters.
This can only be achieved in general is by omitting observations, then refitting the model.

\citet{schabenberger} describes the choice between \index{iterative influence analysis} iterative influence analysis and \index{non-iterative influence analysis} non-iterative influence analysis.



\section{Iterative and non-iterative influence analysis} %1.13
\citet{schabenberger} highlights some of the issue regarding implementing mixed model diagnostics.


A measure of total influence requires updates of all model parameters.


however, this doesnt increase the procedures execution time by the same degree.

\section{Analysis of  Influence}


%--------------------------------------%

\subsection{Further Assumptions of Linear Models}

As with fitted models, the assumption of normality of residuals and homogeneity of variance is applicable to LMEs also. 

%--------------------------------------%


Homoscedascity is the technical term to describe the variance of the residuals being constant across the range of predicted values.
Heteroscedascity is the converse scenario : the variance differs along the range of values.

%--Marginal and Conditional Residuals

% \subfile{ResidualsLMEs.tex}
% \subfile{iterativemethods.tex}



In recent years, mixed models have become invaluable tools in the analysis of experimental and observational
data. In these models, more than one term can be subject to random variation. Mixed model technology enables you to analyze complex experimental data with hierarchical random processes, temporal,
longitudinal, and spatial data, to name just a few important applications. 


%====================================================================================================================%
\subsection{Summary of Schabenberger's Paper}

On occasion, quantification is not possible. Assume, for example, that a data point is removed
and the new estimate of the G matrix is not positive definite. This may occur if a variance component
estimate now falls on the boundary of the parameter space. Thus, it may not be possible to compute certain
influence statistics comparing the full-data and reduced-data parameter estimates. However, knowing that
a new singularity was encountered is important qualitative information about the data point’s influence on
the analysis.

We use the subscript (U) to denote quantities obtained without the observations in the set U. For example,
%βb
(U) denotes the fixed-effects “\textit{\textbf{leave-U-out}}” estimates. Note that the set U can contain multiple observations.


%===================================================================================
If the global measure suggests that the points in U are influential, you should next determine the nature of
that influence. In particular, the points can affect
\begin{itemize}
	\item the estimates of fixed effects
	\item the estimates of the precision of the fixed effects
	\item the estimates of the covariance parameters
	\item the estimates of the precision of the covariance parameters
	\item fitted and predicted values
\end{itemize}

It is important to further decompose the initial finding to determine whether data points are actually troublesome.
Simply because they are influential “somehow”, should not trigger their removal from the analysis or
a change in the model. For example, if points primarily affect the precision of the covariance parameters
without exerting much influence on the fixed effects, then their presence in the data may not distort hypothesis
tests or confidence intervals about $\beta$.
%They will only do so if your inference depends on an estimate of the
%precision of the covariance parameter estimates, as is the case for the Satterthwaite and Kenward-Roger
%degrees of freedom methods and the standard error adjustment associated with the DDFM=KR option.

%------------------------------------------------------------%
\subsection{Summary of Paper}

Standard residual and influence diagnostics for linear models can be extended to LME models.
The dependence of the fixed effects solutions on the covariance parameters has important ramifications on the perturbation analysis.	
Calculating the studentized residuals-And influence statistics whereas each software procedure can calculate both conditional and marginal raw residuals, only SAs Proc Mixed is currently the only program that provide studentized residuals Which ave preferred for model diagnostics. The conditional Raw residuals ave not well suited to detecting outliers as are the studentized conditional residuals. (schabenbege r)


LME are flexible tools for the analysis of clustered and repeated measurement data. LME extend the capabilities of standard linear models by allowing unbalanced and missing data, as long as the missing data are MAR. Structured covariance matrices for both the random effects G and the residuals R. missing at Random.

A conditional residual is the difference between the observed valve and the predicted valve of a dependent variable- Influence diagnostics are formal techniques that allow the identification observation that heavily influence estimates of parameters.
To alleviate the problems with the interpretation of conditional residuals that may have unequal variances, we consider sealing.
Residuals obtained in this manner ave called studentized residuals.

\begin{itemize}
	\item Standard residual and inﬂuence diagnostics for linear models can be extended to linear mixed models. The dependence of ﬁxed-effects solutions on the covariance parameter estimates has important ramiﬁcations in perturbation analysis. 
	\item To gauge the full impact of a set of observations on the analysis, covariance parameters need to be updated, which requires reﬁtting of the model. 
	%	\item The experimental INFLUENCE option of the MODEL statement in the MIXED procedure (SAS 9.1) enables you to perform iterative and noniterative inﬂuence analysis for individual observations and sets of observations.
	
	\item The conditional (subject-speciﬁc) and marginal (population-averaged) formulations in the linear mixed model enable you to consider conditional residuals that use the estimated BLUPs of the random effects, and marginal residuals which are deviations from the overall mean. 
	\item Residuals using the BLUPs are useful to diagnose whether the random effects components in the model are speciﬁed correctly, marginal residuals are useful to diagnose the ﬁxed-effects components. 
	\item Both types of residuals are available in SAS 9.1 as an experimental option of the MODEL statement in the MIXED procedure.
	
	\item It is important to note that influence analyses are performed under the assumption that the chosen model is correct. Changing the model structure can alter the conclusions. Many other variance models have been ﬁt to the data presented in the repeated measures example. You need to see the conclusions about which model component is affected in light of the model being fit.
	%	\item  For example, modeling these data with a random intercept and random slope for each child or an unstructured covariance matrix will affect your conclusions about which children are inﬂuential on the analysis and how this influence manifests itself.
\end{itemize}






\subsection{Cook's 1986 paper on Local Influence}%1.7.1
Cook 1986 introduced methods for local influence assessment. These methods provide a powerful tool for examining perturbations in the assumption of a model, particularly the effects of local perturbations of parameters of observations.


The local-influence approach to influence assessment is quitedifferent from the case deletion approach, comparisons are of
interest.

	\subsection{Influence}
	
	
	Broadly defined, influence is understood as the ability of a single or multiple data points, through their presence or absence in the data, to alter important aspects of the analysis, yield qualitatively different inferences, or violate assumptions of the statistical model. The goal of influence analysis is not primarily to mark data
	points for deletion so that a better model fit can be achieved for the reduced data, although this might be a result of influence analysis \citep{schabenberger}.			
	The goal of influence analysis is not primarily to mark data
	points for deletion so that a better model fit can be achieved for the reduced data, although this might be a
	result of influence analysis. The goal is rather to determine which cases are influential and the manner in
	which they are important to the analysis. Outliers, for example, may be the most noteworthy data points in
	an analysis. They can point to a model breakdown and lead to development of a better model.

	\subsection{Influence Analysis for LME Models} %1.1.3
			The linear mixed effects model is a useful methodology for fitting a wide range of models. However, linear mixed effects models are known to be sensitive to outliers. \citet{CPJ} advises that identification of outliers is necessary before conclusions may be drawn from the fitted model.
			
			Standard statistical packages concentrate on calculating and testing parameter estimates without considering the diagnostics of the model.The assessment of the effects of perturbations in data, on the outcome of the analysis, is known as statistical influence analysis. Influence analysis examines the robustness of the model. Influence analysis methodologies have been used extensively in classical linear models, and provided the basis for methodologies for use with LME models.
			Computationally inexpensive diagnostics tools have been developed to examine the issue of influence \citep{Zewotir}.
			Studentized residuals, error contrast matrices and the inverse of the response variance covariance matrix are regular components of these tools.
			
			Influence arises at two stages of the LME model. Firstly when $V$ is estimated by $\hat{V}$, and subsequent
			estimations of the fixed and random regression coefficients $\beta$ and $u$, given $\hat{V}$.
			
			%-------------------------------------------------------------------------------------------------------------------------------------%
			
			
			%	\newpage
			%	\section{Covariance Parameters} %1.5
			%	The unknown variance elements are referred to as the covariance parameters and collected in the vector $\theta$.
			% - where is this coming from?
			% - where is it used again?
			% - Has this got anything to do with CovTrace etc?
			%---------------------------------------------------------------------------%

			\subsection*{Influence Diagnostics: Basic Idea and Statistics} %1.1.2
			%http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mixed_sect024.htm
			
			The general idea of quantifying the influence of one or more observations relies on computing parameter estimates based on all data points, removing the cases in question from the data, refitting the model, and computing statistics based on the change between full-data and reduced-data estimation. 
			
			\subsection{Residual diagnostics} %1.3
			For classical linear models, residual diagnostics are typically implemented as a plot of the observed residuals and the predicted values. A visual inspection for the presence of trends inform the analyst on the validity of distributional assumptions, and to detect outliers and influential observations.
			
			
			\subsection*{Extension of techniques to LME Models} %1.2
			
			Model diagnostic techniques, well established for classical models, have since been adapted for use with linear mixed effects models.Diagnostic techniques for LME models are inevitably more difficult to implement, due to the increased complexity.
			
			% - \citet{Beckman}
			Beckman, Nachtsheim and Cook (1987)  applied the \index{local influence}local influence method of Cook (1986) to the analysis of the linear mixed model.
			
			While the concept of influence analysis is straightforward, implementation in mixed models is more complex. Update formulae for fixed effects models are available only when the covariance parameters are assumed to be known.
			
			If the global measure suggests that the points in $U$ are influential, the nature of that influence should be determined. In particular, the points in $U$ can affect the following
			
			\begin{itemize}
				\item the estimates of fixed effects,
				\item the estimates of the precision of the fixed effects,
				\item the estimates of the covariance parameters,
				\item the estimates of the precision of the covariance parameters,
				\item fitted and predicted values.
			\end{itemize}
			

			
			\subsection{Influence Statistics for LME models} %1.1.4
			Influence statistics can be coarsely grouped by the aspect of estimation that is their primary target:
			\begin{itemize}
				\item overall measures compare changes in objective functions: (restricted) likelihood distance (Cook and Weisberg 1982, Ch. 5.2)
				\item influence on parameter estimates: Cook's  (Cook 1977, 1979), MDFFITS (Belsley, Kuh, and Welsch 1980, p. 32)
				\item influence on precision of estimates: CovRatio and CovTrace
				\item influence on fitted and predicted values: PRESS residual, PRESS statistic (Allen 1974), DFFITS (Belsley, Kuh, and Welsch 1980, p. 15)
				\item outlier properties: internally and externally studentized residuals, leverage
			\end{itemize}
			%---------------------------------------------------------------------------%
			
%%	Beckman, Nachtsheim and Cook (1987)
			
			 \citet{Beckman} applied the \index{local influence}local influence method of Cook (1986) to the analysis of the linear mixed model.
			
			While the concept of influence analysis is straightforward, implementation in mixed models is more complex. Update formulae for fixed effects models are available only when the covariance parameters are assumed to be known.
			
			If the global measure suggests that the points in $U$ are influential, the nature of that influence should be determined. In particular, the points in $U$ can affect the following
			
			\begin{itemize}
				\item the estimates of fixed effects,
				\item the estimates of the precision of the fixed effects,
				\item the estimates of the covariance parameters,
				\item the estimates of the precision of the covariance parameters,
				\item fitted and predicted values.
			\end{itemize}
			%---------------------------------------------------------------------------%

			
			
			
			
			\subsection{Influence}
			The influence of an observation can be thought of in terms of how much the predicted scores for other observations would differ if the observation in question were not included. 
			
			Cook's D is a good measure of the influence of an observation and is proportional to the sum of the squared differences between predictions made with all observations in the analysis and predictions made leaving out the observation in question. If the predictions are the same with or without the observation in question, then the observation has no influence on the regression model. If the predictions differ greatly when the observation is not included in the analysis, then the observation is influential.
			
			\subsection{Leverage}
			% http://onlinestatbook.com/2/regression/influential.html
			% Leverage
			The leverage of an observation is based on how much the observation's value on the predictor variable differs from the mean of the predictor variable. The greater an observation's leverage, the more potential it has to be an influential observation. 
			
			For example, an observation with a value equal to the mean on the predictor variable has no influence on the slope of the regression line regardless of its value on the criterion variable. On the other hand, an observation that is extreme on the predictor variable has the potential to affect the slope greatly.
			
			\subsubsection{Calculation of Leverage (h)}
			The first step is to standardize the predictor variable so that it has a mean of 0 and a standard deviation of 1. Then, the leverage (h) is computed by squaring the observation's value on the standardized predictor variable, adding 1, and dividing by the number of observations.
			
			
			\subsection{Summary of Influence Statistics}
			\begin{itemize}
				\item	\textbf{Studentized Residuals} – Residuals divided by their estimated standard errors (like t-statistics). Observations with values larger than 3 in absolute value are considered outliers.
				\item	\textbf{Leverage Values (Hat Diag)} – Measure of how far an observation is from the others in terms of the levels of the independent variables (not the dependent variable). Observations with values larger than $2(k+1)/n$ are considered to be potentially highly influential, where k is the number of predictors and n is the sample size.
				\item	\textbf{DFFITS} – Measure of how much an observation has effected its fitted value from the regression model. Values larger than $2\sqrt{(k+1)/n}$ in absolute value are considered highly influential. %Use standardized DFFITS in SPSS.
				\item	\textbf{DFBETAS} – Measure of how much an observation has effected the estimate of a regression coefficient (there is one DFBETA for each regression coefficient, including the intercept). Values larger than 2/sqrt(n) in absolute value are considered highly influential.
				\\
				The measure that measures how much impact each observation has on a particular predictor is DFBETAs The DFBETA for a predictor and for a particular observation is the difference between the regression coefficient calculated for all of the data and the regression coefficient calculated with the observation deleted, scaled by the standard error calculated with the observation deleted. 
				
				\item	\textbf{Cook’s D} – Measure of aggregate impact of each observation on the group of regression coefficients, as well as the group of fitted values. Values larger than 4/n are considered highly influential.
			\end{itemize}
			
			
			\subsection{Interpreting Cook's Distance}
			A common rule of thumb is that an observation with a value of Cook's D over 1.0 has too much influence. As with all rules of thumb, this rule should be applied judiciously and not thoughtlessly.

			\subsection{Iterative Influence Analysis}
			
			
			%----schabenberger page 8
			For linear models, the implementation of influence analysis is straightforward.
			However, for LME models, the process is more complex. Update formulas for the fixed effects are available only when the covariance parameters are assumed to be known. A measure of total influence requires updates of all model parameters.
			This can only be achieved in general is by omitting observations, then refitting the model.
			
			
			\citet{schabenberger} describes the choice between \index{iterative influence analysis} iterative influence analysis and \index{non-iterative influence analysis} non-iterative influence analysis.
			
			
			\section{Influence analysis} %1.7
			
			
			Likelihood based estimation methods, such as ML and REML, are sensitive to unusual observations. Influence diagnostics are formal techniques that assess the influence of observations on parameter estimates for $\beta$ and $\theta$. A common technique is to refit the model with an observation or group of observations omitted.
			
			
			\citet{west} examines a group of methods that examine various aspects of influence diagnostics for LME models.
			For overall influence, the most common approaches are the `likelihood distance' and the `restricted likelihood distance'.
			
			
			
			
			\section*{Diagnostic Methods for OLS models}
			% Cook's Distance for OLS models
			% http://www.amstat.org/meetings/jsm/2012/onlineprogram/AbstractDetails.cfm?abstractid=305411
			Influence diagnostics are formal techniques allowing for the identification of observations that exert substantial 
			influence on the estimates of fixed effects and variance covariance parameters. 
			
			The idea of influence diagnostics for a given observation is to quantify the effect of omission of this observation 
			from the data on the results of the model fit. To this aim, the concept of likelihood displacement is used. 
			
			%---------------------------------------------------------------%
			% We have developed a function in R, which allows performing influence diagnostics for linear mixed effects models 
			% fitted using the lme() function from the nlme package. 
			% The use of the new function is illustrated using data from a randomized clinical trial.
			
			%---------------------------------------------------------------%
			
			
			
			\section{Effects on fitted and predicted values}
			\begin{equation}
			\hat{e_{i}}_{(U)} = y_{i} - x\hat{\beta}_{(U)}
			\end{equation}
			

			
			\subsection{Marginal Residuals}
			\begin{eqnarray}
			\hat{\beta} &=& (X^{T}R^{-1}X)^{-1}X^{T}R^{-1}Y \nonumber \\
			&=& BY \nonumber
			\end{eqnarray}
			
			
			\section{Covariance Parameters} %1.5
			The unknown variance elements are referred to as the covariance parameters and collected in the vector $\theta$.
			% - where is this coming from?
			% - where is it used again?
			% - Has this got anything to do with CovTrace etc?
			%---------------------------------------------------------------------------%
			
			\subsection{Methods and Measures}
			The key to making deletion diagnostics useable is the development of efficient computational formulas, allowing one to obtain the \index{case deletion diagnostics} case deletion diagnostics by making use of basic building blocks, computed only once for the full model.
			
			\citet{Zewotir} lists several established methods of analyzing influence in LME models. These methods include \begin{itemize}
				\item Cook's distance for LME models,
				\item \index{likelihood distance} likelihood distance,
				\item the variance (information) ration,
				\item the \index{Cook-Weisberg statistic} Cook-Weisberg statistic,
				\item the \index{Andrews-Prebigon statistic} Andrews-Prebigon statistic.
			\end{itemize}
			
			
			%=========================================================================%
			\section{Model Validation Framework}
			%\section{Model Validation using Residual Diagnostics}
			In statistical modelling, the process of model validation is a critical step of model fitting process, but also a step that is too often overlooked. A very simple procedure is to examine commonly-used
			metrics, such as the $R^2$ value. However, using a small handful of simple measures and methods is insufficient to properly assess the quality of a fitted model. To do so properly, a full and comprehensive
			analysis that tests of all of the assumptions, as far as possible, must be carried out.
			
			
			A statistical model, whether of the fixed-effects or mixed-effects variety, represents how you think your data
			were generated. Following model specification and estimation, it is of interest to explore the model-data
			agreement by raising questions such as
			\begin{itemize}
				\item Does the model-data agreement support the model assumptions?
				\item Should model components be refined, and if so, which components? For example, should regressors
				be added or removed, and is the covariation of the observations modeled properly?
				\item Are the results sensitive to model and/or data? Are individual data points or groups of cases particularly
				influential on the analysis?
			\end{itemize}

			
			%=========================================================================%
			%\subsection{Model Validation Framework}
			%In classical linear models, this examination of model-data agreement has traditionally revolved around
			\citet{schab} describes the model validatin framework as comprised of the following tasks
			
			\begin{itemize}
				\item  overall measures of goodness-of-fit
				\item the informal, graphical examination of estimates of model errors to assess the quality of distributional
				assumptions: residual analysis
				
				
				\item the quantitative assessment of the inter-relationship of model components; for example, collinearity 	diagnostics
				\item the qualitative and quantitative assessment of influence of cases on the analysis, i.e. influence analysis.
			\end{itemize}
			
			The sensitivity of a model is studied through measures that express its stability under perturbations. You
			are not interested in a model that is either overly stable or overly sensitive. Changes in the data or model
			components should produce commensurate changes in the model output. The difficulty is to determine
			when the changes are substantive enough to warrant further investigation, possibly leading to a reformulation
			of the model or changes in the data (such as dropping outliers). This paper is primarily concerned
			with stability of linear mixed models to perturbations of the data; that is, with influence analysis. 

\subsection{Residual Analysis}
			A residual is the difference between an observed quantity and its
			estimated or predicted value. 
			Residual analysis is a widely used model validation technique. A residual is simply the difference between an observed value and the corresponding fitted value, as predicted by the model. The rationale is that, if the model is properly fitted to the model, then the residuals would approximate the random errors that one should expect.
			that is to say, if the residuals behave randomly, with no discernible trend, the model has fitted the data well. If some sort of non-random trend is evident in the model, then the model can be considered to be poorly fitted.
			Statistical software environments, such as the \texttt{R} Programming language, provides a suite of tests and graphical procedure sfor appraising a fitted linear model, with several 
			of these procedures analysing the model residuals.
			
			In classical linear models, an examination of model-data agreement has traditionally revolved around
			
			The second part of the chapter looks at diagnostics techniques for LME models, firsly covering the theory, then proceeding to a discussion on 
			implementing these using \texttt{R} code.
			
			While a substantial body of work has been developed in this area, there is still areas worth exploring. 
			In particular the development of graphical techniques pertinent to LME models should be looked at.
			

			
			%========================================================================================================= %
			%\subsection{Introduction}
			%A statistical model, whether of the fixed-effects or mixed-effects variety, represents how you think your data were generated. 
			%Following model specification and estimation, it is of interest to explore the model-data
			%agreement by raising questions such as
			
			Statistical software environments, such as the \texttt{R} Programming language, provides a suite of tests and graphical procedure sfor appraising a fitted linear model, with several 
			of these procedures analysing the model residuals.
			
			
			
			
			
			%========================================================================================================= %
			\subsection{Outliers and Leverage}
			
			
			
			The question of whether or not a point should be considered an outlier must also be addressed. An outlier is an observation whose true value is unusual given its value on the predictor variables. The leverage of an observation is a further consideration. Leverage describes an observation with an extreme value on a predictor variable is a point with high leverage. High leverage points can have a great amount of effect on the estimate of regression coefficients.
			% - Leverage is a measure of how far an independent variable deviates from its mean.
			
			Influence can be thought of as the product of leverage and outlierness. An observation is said to be influential if removing the observation substantially changes the estimate of the regression coefficients. The \texttt{R} programming language has a variety of methods used to study each of the aspects for a linear model. While linear models and GLMS can be studied with a wide range of well-established diagnostic technqiues, the choice of methodology is much more restricted for the case of LMEs.
			
			%---------------------------------------------------------------------------%
			%\newpage
			%\section{Residual diagnostics} %1.3
			For classical linear models, residual diagnostics are typically conducted using a plot of the observed residuals and the predicted values. A visual inspection for the presence of trends inform the analyst on the validity of distributional assumptions, and to detect outliers and influential observations.
			
			%\section{Case Deletion Diagnostics}
			%
			%
			%Linear models for uncorrelated data have well established measures to gauge the influence of one or more
			%observations on the analysis. For such models, closed-form update expressions allow efficient computations
			%without refitting the model. 
			%
			%
			%Since the pioneering work of Cook in 1977, deletion measures have been applied to many statistical models for identifying influential observations. Case-deletion diagnostics provide a useful tool for identifying influential observations and outliers.
			%
			%The key to making deletion diagnostics useable is the development of efficient computational formulas, allowing one to obtain the \index{case deletion diagnostics} case deletion diagnostics by making use of basic building blocks, computed only once for the full model.
			%
			%The computation of case deletion diagnostics in the classical model is made simple by the fact that estimates of $\beta$ and $\sigma^2$, which exclude the $i-$th observation, can be computed without re-fitting the model. %\subsection{Terminology for Case Deletion diagnostics} %1.8
			%
			%\citet{preisser} describes two type of diagnostics. When the set consists of only one observation, the type is called
			%`\textit{observation-diagnostics}'. For multiple observations, Preisser describes the diagnostics as `\textit{cluster-deletion}' diagnostics. When applied to LME models, such update formulas are available only if one assumes that the covariance parameters are not affected by the removal of the observation in question. However, this is rarely a reasonable assumption.
			%
			%
			%
			%
			%%---------------------------------------------------------------------------%
			
			\subsection{Extension of Diagnostic Methods to LME models}
			
			
			When similar notions of statistical influence are applied to mixed models,
			things are more complicated. Removing data points affects fixed effects and covariance parameter estimates.
			Update formulas for “\textit{leave-one-out}” estimates typically fail to account for changes in covariance
			parameters. 
			%
			%
			%In LME models, there are two types of residuals, marginal residuals and conditional residuals. A
			%marginal residual is the difference between the observed data and the estimated marginal mean. A conditional residual is the
			%difference between the observed data and the predicted value of the observation. In a model without random effects, both sets of residuals coincide \citep{schab}.
			
			\citet{Christiansen} noted the case deletion diagnostics techniques have not been applied to linear mixed effects models and seeks to develop 
			\citet{CPJ} noted the case deletion diagnostics techniques had not been applied to linear mixed effects models and seeks to develop methodologies in that respect. \citet{CPJ} develops these techniques in the context of REML.
			
			
			%\citet{CPJ} develops \index{case deletion diagnostics} case deletion diagnostics, in particular the equivalent of \index{Cook's distance} Cook's distance, a well-known metric, for diagnosing influential observations when estimating the fixed effect parameters and variance components. Deletion diagnostics provide a means of assessing the influence of an observation (or groups of observations) on inference on the estimated parameters of LME models. We shall provide a fuller discussion of Cook's distance in due course.
			
			
			\citet{Demi} extends several regression diagnostic techniques commonly used in linear regression, such as leverage, infinitesimal influence, case deletion diagnostics, Cook's distance, and local influence to the linear mixed-effects model. In each case, the proposed new measure has a direct interpretation in terms of the effects on a parameter of interest, and reduces to the familiar linear regression measure when there are no random effects. 
			
			The new measures that are proposed by \citet{Demi} are explicitly defined functions and do not require re-estimation of the model, especially for cluster deletion diagnostics. The basis for both the cluster deletion diagnostics and Cook's distance is a generalization of Miller's simple update formula for case deletion for linear models. Furthermore \citet{Demi} shows how Pregibon's infinitesimal case deletion diagnostics is adapted to the linear mixed-effects model. 
			%A simple compact matrix formula is derived to assess the local influence of the fixed-effects regression coefficients. 
			
			
			%
			%
			%\section{Case Deletion Diagnostics for LME models} %1.6
			%
			%Data from single individuals, or a small group of subjects may influence non-linear mixed effects model selection. Diagnostics routinely applied in model building may identify such individuals, but these methods are not specifically designed for that purpose and are, therefore, not optimal. 
			
			\citet{Demi} proposes two likelihood-based diagnostics for identifying individuals that can influence the choice between two competing models.
			
			
			
			\section{Analysis of  Influence}
			
			
			
			\subsection{Further Assumptions of Linear Models}
			
			As with fitted models, the assumption of normality of residuals and homogeneity of variance is applicable to LMEs also. 
			
			%--------------------------------------%
			
			
			Homoscedascity is the technical term to describe the variance of the
			residuals being constant across the range of predicted values.
			Heteroscedascity is the converse scenario : the variance differs along
			the range of values.
			
			%--Marginal and Conditional Residuals
			
			
			In recent years, mixed models have become invaluable tools in the analysis of experimental and observational
			data. In these models, more than one term can be subject to random variation. Mixed model
			technology enables you to analyze complex experimental data with hierarchical random processes, temporal,
			longitudinal, and spatial data, to name just a few important applications. 
			
			
			\subsection{Summary of Schabenberger's Paper}
			
			
			%\subsection{INFLUENCE DIAGNOSTICS IN THE MIXED PROCEDURE}
			%Key to the implementations of influence diagnostics in the MIXED procedure is the attempt to quantify
			%influence, where possible, by drawing on the basic definitions of the various statistics in the classical linear
			%model. 
			
			On occasion, quantification is not possible. Assume, for example, that a data point is removed
			and the new estimate of the G matrix is not positive definite. This may occur if a variance component
			estimate now falls on the boundary of the parameter space. Thus, it may not be possible to compute certain
			influence statistics comparing the full-data and reduced-data parameter estimates. However, knowing that
			a new singularity was encountered is important qualitative information about the data point’s influence on
			the analysis.
	
			%===================================================================================
			If the global measure suggests that the points in U are influential, you should next determine the nature of
			that influence. In particular, the points can affect
			\begin{itemize}
				\item the estimates of fixed effects
				\item the estimates of the precision of the fixed effects
				\item the estimates of the covariance parameters
				\item the estimates of the precision of the covariance parameters
				\item fitted and predicted values
			\end{itemize}
			
			It is important to further decompose the initial finding to determine whether data points are actually troublesome.
			Simply because they are influential “somehow”, should not trigger their removal from the analysis or
			a change in the model. For example, if points primarily affect the precision of the covariance parameters
			without exerting much influence on the fixed effects, then their presence in the data may not distort hypothesis
			tests or confidence intervals about $\beta$.
			%They will only do so if your inference depends on an estimate of the
			%precision of the covariance parameter estimates, as is the case for the Satterthwaite and Kenward-Roger
			%degrees of freedom methods and the standard error adjustment associated with the DDFM=KR option.
			
			%------------------------------------------------------------%
			\subsection{Summary of Paper}
			
			%Summary of Schabenberger
			Standard residual and influence diagnostics for linear models can be extended to LME models.
			The dependence of the fixed effects solutions on the covariance parameters has important ramifications on the perturbation analysis.	
			Calculating the studentized residuals-And influence statistics whereas each software procedure can calculate both conditional and marginal raw residuals, only SAs Proc Mixed is currently the only program that provide studentized residuals Which ave preferred for model diagnostics. The conditional Raw residuals ave not well suited to detecting outliers as are the studentized conditional residuals. (schabenbege r)
			
			
			LME are flexible tools for the analysis of clustered and repeated measurement data. LME extend the capabilities of standard linear models by allowing unbalanced and missing data, as long as the missing data are MAR. Structured covariance matrices for both the random effects G and the residuals R. missing at Random.
			
			A conditional residual is the difference between the observed valve and the predicted valve of a dependent variable- Influence diagnostics are formal techniques that allow the identification observation that heavily influence estimates of parameters.
			To alleviate the problems with the interpretation of conditional residuals that may have unequal variances, we consider sealing.
			Residuals obtained in this manner ave called studentized residuals.
			
			\begin{itemize}
				\item Standard residual and inﬂuence diagnostics for linear models can be extended to linear mixed models. The dependence of ﬁxed-effects solutions on the covariance parameter estimates has important ramiﬁcations in perturbation analysis. 
				\item To gauge the full impact of a set of observations on the analysis, covariance parameters need to be updated, which requires reﬁtting of the model. 
				%	\item The experimental INFLUENCE option of the MODEL statement in the MIXED procedure (SAS 9.1) enables you to perform iterative and noniterative inﬂuence analysis for individual observations and sets of observations.
				
				\item The conditional (subject-speciﬁc) and marginal (population-averaged) formulations in the linear mixed model enable you to consider conditional residuals that use the estimated BLUPs of the random effects, and marginal residuals which are deviations from the overall mean. 
				\item Residuals using the BLUPs are useful to diagnose whether the random effects components in the model are speciﬁed correctly, marginal residuals are useful to diagnose the ﬁxed-effects components. 
				\item Both types of residuals are available in SAS 9.1 as an experimental option of the MODEL statement in the MIXED procedure.
				
				\item It is important to note that influence analyses are performed under the assumption that the chosen model is correct. Changing the model structure can alter the conclusions. Many other variance models have been ﬁt to the data presented in the repeated measures example. You need to see the conclusions about which model component is affected in light of the model being fit.
				%	\item  For example, modeling these data with a random intercept and random slope for each child or an unstructured covariance matrix will affect your conclusions about which children are inﬂuential on the analysis and how this influence manifests itself.
			\end{itemize}
			
			
			
			
			
			\section{Influence in LME Models}
			
			Model diagnostic techniques, well established for classical models, have since been adapted for use with linear mixed effects models. Diagnostic techniques for LME models are inevitably more difficult to implement, due to the increased complexity.
			

\subsection{Key Definitions}
\begin{description}
	\item[ Residual] The difference between the predicted value (based on the regression equation) and the actual, observed value.
	
	\item[ Outlier] In linear regression, an outlier is an observation with large residual. In other words, it is an observation whose dependent-variable value is unusual given its value on the predictor variables. An outlier may indicate a sample peculiarity or may indicate a data entry error or other problem.
	
	\item[ Leverage] An observation with an extreme value on a predictor variable is a point with high leverage. Leverage is a measure of how far an independent variable deviates from its mean. High leverage points can have a great amount of effect on the estimate of regression coefficients.
	
	\item[ Influence] An observation is said to be influential if removing the observation substantially changes the estimate of the regression coefficients.  Influence can be thought of as the product of leverage and outlierness.
	
	\item[ Cook's distance ] A measure that combines the information of leverage and residual of the observation.
\end{description}

%--------------------- %

\subsection{Leverage}
In statistics, leverage is a term used in connection with regression analysis and, in particular, in analyses aimed at identifying those observations that are far away from corresponding average predictor values. Leverage points do not necessarily have a large effect on the outcome of fitting regression models.



Leverage points are those observations, if any, made at extreme or outlying values of the independent variables such that the lack of neighboring observations means that the fitted regression model will pass close to that particular observation.

Modern computer packages for statistical analysis include, as part of their facilities for regression analysis, various quantitative measures for identifying influential observations: among these measures is partial leverage, a measure of how a variable contributes to the leverage of a datum.




\subsection{Leverage in LME models}
% Schabenberger

For the general mixed model, leverage can be defined through the projection matrix that results from a transformation of the model with the inverse of the Cholesky decomposition of , or through an oblique projector \citep{schabenberger}. The MIXED procedure follows the latter path in the computation of influence diagnostics. 

%---------------------------------------------------------------------- %

The leverage value reported for the th observation is the th diagonal entry of the matrix

which is the weight of the observation in contributing to its own predicted value, .
While  is idempotent, it is generally not symmetric and thus not a projection matrix in the narrow sense.

The properties of these leverages are generalizations of the properties in models with diagonal variance-covariance matrices. For example, , and in a model with intercept and , the leverage values

are  and . The lower bound for  is achieved in an intercept-only model, and the upper bound is achieved in a saturated model. 

The trace of  equals the rank of .
If  denotes the element in row , column  of , then for a model containing only an intercept the diagonal elements of  are

Because  is a sum of elements in the th row of the inverse variance-covariance matrix,  can be negative, even if the correlations among data points are nonnegative. In case of a saturated model with , .			
			
	%---------------------------------------------------------------------------%
	\newpage
	\section{Iterative and non-iterative influence analysis} %1.13
	\citet{schab} highlights some of the issue regarding implementing mixed model diagnostics.
	
	% A measure of total influence requires updates of all model parameters.
	% however, this doesnt increase the procedures execution time by the same degree.
	
	\subsection{Iterative Influence Analysis}
	
	%----schabenberger page 8
	For linear models, the implementation of influence analysis is straightforward.
	However, for LME models, the process is more complex. Update formulas for the fixed effects are available only when the covariance parameters are assumed to be known. A measure of total influence requires updates of all model parameters.
	This can only be achieved in general is by omitting observations, then refitting the model.
	
	\citet{schab} describes the choice between \index{iterative influence analysis} iterative influence analysis and \index{non-iterative influence analysis} non-iterative influence analysis.
	
	
	
	
	
	\subsection{Iterative vs Non-Iterative Influence Analysis}
	%\subsection{ITERATIVE VS. NONITERATIVE INFLUENCE ANALYSIS}
	While the basic idea of influence analysis is straightforward, the implementation in mixed models can be
	tricky. For example, update formulas for the fixed effects are available only when the covariance parameters
	are assumed to be known. At most the profiled residual variance can be updated without refitting the model.
	
	A measure of total influence requires updates of all model parameters, and the only way that this can be
	achieved in general is by removing the observations in question and refitting the model. 
	
	Because this “\textbf{bruteforce}”
	method involves iterative reestimation of the covariance parameters, it is termed \textbf{\textit{iterative influence
			analysis}}. Reliance on closed-form update formulas for the fixed effects without updating the (un-profiled)
	covariance parameters is termed a noniterative influence analysis.
	
	An iterative analysis seems like a costly, computationally intensive enterprise. If you compute iterative
	influence diagnostics for all n observations, then a total of $n + 1$ mixed models are fit iteratively. This does
	not imply, of course, that the procedure’s execution time increases n-fold. Keep in mind that
	\begin{itemize}
		\item iterative reestimation always starts at the converged full-data estimates. If a data point is not influential,
		then its removal will have little effect on the objective function and parameter estimates. Within
		one or two iterations, the process should arrive at the reduced-data estimates.
		\item if complete reestimation does require many iterations, then this is important information in itself. The
		likelihood surface has probably changed drastically, and the reduced-data estimates are moving away
	\end{itemize}
	from the full-data estimates.
			
			\bibliographystyle{chicago}
			\bibliography{DB-txfrbib}
		\end{document}